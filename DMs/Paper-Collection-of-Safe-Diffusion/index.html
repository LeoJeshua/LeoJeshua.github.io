<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="LeoJeshua">
    
    <!-- Completely eliminate flash of wrong theme -->
    <script>
        (function() {
            const THEME_KEY = "REDEFINE-THEME-STATUS";
            const DARK = "dark", LIGHT = "light";
            
            // Get preferred theme
            function getTheme() {
                try {
                    const saved = localStorage.getItem(THEME_KEY);
                    if (saved) {
                        const { isDark } = JSON.parse(saved);
                        return isDark ? DARK : LIGHT;
                    }
                } catch (e) {}
                
                return matchMedia("(prefers-color-scheme: dark)").matches ? DARK : LIGHT;
            }
            
            // Apply theme to document
            function applyTheme(theme) {
                const isDark = theme === DARK;
                const root = document.documentElement;
                
                // Set data attribute for CSS variables
                root.setAttribute("data-theme", theme);
                
                // Set classes for compatibility
                root.classList.add(theme);
                root.classList.remove(isDark ? LIGHT : DARK);
                root.style.colorScheme = theme;
            }
            
            // Initial application
            const theme = getTheme();
            applyTheme(theme);
            
            // Listen for system preference changes
            matchMedia("(prefers-color-scheme: dark)").addEventListener("change", ({ matches }) => {
                // Only update if using system preference (no localStorage entry)
                if (!localStorage.getItem(THEME_KEY)) {
                    applyTheme(matches ? DARK : LIGHT);
                }
            });
            
            // Set body classes once DOM is ready
            if (document.readyState !== "loading") {
                document.body.classList.add(theme + "-mode");
            } else {
                document.addEventListener("DOMContentLoaded", () => {
                    document.body.classList.add(theme + "-mode");
                    document.body.classList.remove((theme === DARK ? LIGHT : DARK) + "-mode");
                });
            }
        })();
    </script>
    
    <!-- Critical CSS to prevent flash -->
    <style>
        :root[data-theme="dark"] {
            --background-color: #202124;
            --background-color-transparent: rgba(32, 33, 36, 0.6);
            --second-background-color: #2d2e32;
            --third-background-color: #34353a;
            --third-background-color-transparent: rgba(32, 33, 36, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #ffffff;
            --second-text-color: #eeeeee;
            --third-text-color: #bebec6;
            --fourth-text-color: #999999;
            --default-text-color: #bebec6;
            --invert-text-color: #373D3F;
            --border-color: rgba(255, 255, 255, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(255, 255, 255, 0.08);
            --shadow-color-2: rgba(255, 255, 255, 0.05);
        }
        
        :root[data-theme="light"] {
            --background-color: #fff;
            --background-color-transparent: rgba(255, 255, 255, 0.6);
            --second-background-color: #f8f8f8;
            --third-background-color: #f2f2f2;
            --third-background-color-transparent: rgba(241, 241, 241, 0.6);
            --primary-color: #0066CC;
            --first-text-color: #16171a;
            --second-text-color: #2f3037;
            --third-text-color: #5e5e5e;
            --fourth-text-color: #eeeeee;
            --default-text-color: #373D3F;
            --invert-text-color: #bebec6;
            --border-color: rgba(0, 0, 0, 0.08);
            --selection-color: #0066CC;
            --shadow-color-1: rgba(0, 0, 0, 0.08);
            --shadow-color-2: rgba(0, 0, 0, 0.05);
        }
        
        body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
        
        /* Apply body classes as soon as DOM is ready */
        :root[data-theme="dark"] body {
            background-color: var(--background-color);
            color: var(--default-text-color);
        }
    </style>
    
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
        
        
        
            <link rel="preconnect" href="https://registry.npmmirror.com" crossorigin>
        
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://leojeshua.github.io/dms/paper-collection-of-safe-diffusion/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Collection of Safe Diffusion">
<meta property="og:url" content="https://leojeshua.github.io/DMs/Paper-Collection-of-Safe-Diffusion/index.html">
<meta property="og:site_name" content="The Blog of LeoJeshua">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://leojeshua.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2024-12-21T07:49:09.000Z">
<meta property="article:modified_time" content="2025-11-10T08:42:08.458Z">
<meta property="article:author" content="LeoJeshua">
<meta property="article:tag" content="DMs">
<meta property="article:tag" content="Paper Collection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://leojeshua.github.io/images/redefine-og.webp">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/avatar.png" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.png">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/avatar.png">
    <!--- Page Info-->
    
    <title>
        
            Paper Collection of Safe Diffusion | Academic Blog of LeoJeshua
        
    </title>

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Chillax/chillax.css">

    <!--- Inject Part-->
    
        
            
    
            
    

    
<link rel="stylesheet" href="/css/style.css">


    
        <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/css/build/tailwind.css">
    

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/GeistMono/geist-mono.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fonts/Geist/geist.css">
    <!--- Font Part-->
    
    
    
    
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"leojeshua.github.io","root":"/","language":"en"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"2px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.5rem","h3":"1.8rem","h4":"1.3rem","h5":"1.2rem","h6":"1.1rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":4,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1200px","sidebar_width":"30%","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"side_tools":{"gear_rotation":true,"auto_expand":false},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"static","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"从-1开始的科研训练之路","subtitle":{"text":["If you shed tears when you miss the sun, you also miss the stars.","Reading maketh a full man; conference a ready man; and writing an exact man. (Francis Bacon)","Who drives me forward like fate? The Myself striding on my back."],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/LeoJeshua","instagram":null,"zhihu":null,"twitter":null,"email":"jiaxu.liu.ai@gmail.com"},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.5","navbar":{"auto_hide":true,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Categories":{"path":"/categories/","icon":"fa-regular fa-folder"},"Tags":{"icon":"fa-regular fa-tags","path":"/tags/"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"cloud"},"home":{"sidebar":{"enable":true,"position":"right","first_item":"menu","announcement":"Aspiring to contribute to cutting-edge AI security research with a focus on offensive strategies to uncover and mitigate AI vulnerabilities.","show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2024/12/20 21:33:00"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/fontawesome.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/brands.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/solid.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/fontawesome/regular.min.css">
    
    
    
    
<meta name="generator" content="Hexo 7.3.0"></head>



<body>
	<div class="progress-bar-container">
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Academic Blog of LeoJeshua
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    ARCHIVES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/categories/"
                                        >
                                    <i class="fa-regular fa-folder fa-fw"></i>
                                    CATEGORIES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/tags/"
                                        >
                                    <i class="fa-regular fa-tags fa-fw"></i>
                                    TAGS
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                ARCHIVES
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/categories/"
                        >
                            <span>
                                CATEGORIES
                            </span>
                            
                                <i class="fa-regular fa-folder fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/tags/"
                        >
                            <span>
                                TAGS
                            </span>
                            
                                <i class="fa-regular fa-tags fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">19</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">11</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">15</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			<div class="w-full flex items-center pt-6 justify-start">
				<h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">Paper Collection of Safe Diffusion</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/avatar.png">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">LeoJeshua</span>
					
					<span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv2</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-12-21 15:49:09</span>
        <span class="mobile">2024-12-21 15:49:09</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2025-11-10 16:42:08</span>
            <span class="mobile">2025-11-10 16:42:08</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/AI/">AI</a>&nbsp;
                        </li>
                    
                    
                
                    
                        
                            <li>></li>
                        
                        <li>
                            <a href="/categories/AI/Security-in-AI/">Security in AI</a>&nbsp;
                        </li>
                    
                    
                
                    
                        
                            <li>></li>
                        
                        <li>
                            <a href="/categories/AI/Security-in-AI/DMs/">DMs</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/DMs/">DMs</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/Paper-Collection/">Paper Collection</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>22.9k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>83 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<h1 id="NeurIPS2024"><a href="#NeurIPS2024" class="headerlink" title="NeurIPS2024"></a>NeurIPS2024</h1><p>Submission Deadline: 2024.05.23</p>
<p>Official Site: </p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/papers.html?filter=titles&search=diffusion" >NeurIPS 2024 Papers<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://safegenaiworkshop.github.io/" >Safe Generative AI Workshop @ NeurIPS 2024<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><ul>
<li>openreview: <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/SafeGenAi#tab-accept-oral" >https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/SafeGenAi#tab-accept-oral<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
</li>
</ul>
<p>Tools:</p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://neurips.exa.ai/" >neurips.exa.ai<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://papers.cool/arxiv/search?highlight=1&query=attack+on+diffusion+model" >papers.cool<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h2 id="Attack"><a href="#Attack" class="headerlink" title="Attack"></a>Attack</h2><h3 id="Backdoors-Attack"><a href="#Backdoors-Attack" class="headerlink" title="Backdoors Attack"></a>Backdoors Attack</h3><h4 id="BiBadDiff"><a href="#BiBadDiff" class="headerlink" title="BiBadDiff"></a>BiBadDiff</h4><p><em>From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2311.02373" >https://arxiv.org/pdf/2311.02373<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/OPTML-Group/BiBadDiff" >https://github.com/OPTML-Group/BiBadDiff<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/92999" >https://nips.cc/virtual/2024/poster/92999<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>尽管最先进的扩散模型 (DM) 在图像生成方面表现出色，但对其安全性的担忧依然存在。早期的研究强调了 DM 容易受到数据中毒攻击，但这些研究对图像分类提出了比传统方法 (如 BadNets) 更严格的要求。这是因为技术需要修改扩散训练和取样程序。<br>与之前的工作不同，我们研究了类 BadNet 的数据中毒方法是否可以直接降低 DM 的生成。换句话说，如果只有训练数据集被污染 (没有操纵扩散过程) ，这将如何影响学习的 DM 的性能？在这个设置中，我们揭示了双边数据中毒效应，它不仅服务于对抗目的 (损害 DM 的功能) ，而且还提供了防御优势 (可以在针对中毒攻击的分类任务中利用这一优势进行防御)。<br>我们展示了类似 BadNet 的数据中毒攻击在 DM 中对于产生不正确的图像 (与预期的文本条件不一致) 仍然有效。与此同时，中毒的 DM 在生成的图像中表现出更高的触发比率，这种现象我们称之为 “触发放大”。然后，可以使用这种洞察力来增强中毒训练数据的检测。此外，即使在低中毒率的情况下，研究 DM 的中毒效应对于设计针对此类攻击的鲁棒图像分类器也是有价值的。<br>最后，我们通过研究 DM 固有的数据记忆倾向，在数据中毒和数据复制现象之间建立了有意义的联系。代码可于 <a class="link"   target="_blank" rel="noopener" href="https://github.com/optml-group/bibaddiff" >https://github.com/optml-group/bibaddiff<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 索取。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/92999.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106140414.png"
                      alt="20250106140414"
                ></p>
<h3 id="Adversarial-Attack"><a href="#Adversarial-Attack" class="headerlink" title="Adversarial Attack"></a>Adversarial Attack</h3><h4 id="PAP"><a href="#PAP" class="headerlink" title="PAP"></a>PAP</h4><p><em>Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2408.10571v1" >https://arxiv.org/pdf/2408.10571v1<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>project-page: <a class="link"   target="_blank" rel="noopener" href="https://vancyland.github.io/PAP.github.io/" >https://vancyland.github.io/PAP.github.io/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93631" >https://nips.cc/virtual/2024/poster/93631<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型彻底改变了定制文本到图像的生成，允许高效地从带有文本描述的个人数据合成照片。然而，这些进步带来了风险，包括隐私泄露和未经授权的艺术品复制。<br>先前的研究主要围绕使用特定于提示的方法来生成对抗性示例以保护个人图像，但现有方法的有效性受到对不同提示的适应性限制的阻碍。<br>在本文中，我们介绍了一种用于定制扩散模型的 <strong>prompt无关的对抗扰动 (PAP) 方法</strong>。PAP 首先使用拉普拉斯近似对prompt分布进行建模，然后通过基于建模分布最大化扰动期望来产生即时不可知扰动。这种方法有效地解决了prompt不可知攻击，从而提高了防御稳定性。<br>在人脸隐私和艺术风格保护方面的大量实验表明，与现有技术相比，我们的方法具有更优越的泛化能力。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/93631.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106140739.png"
                      alt="20250106140739"
                ></p>
<h4 id="AdvAD"><a href="#AdvAD" class="headerlink" title="AdvAD"></a>AdvAD</h4><p><em>AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/pdf/edd586d663019327dfd268abca5445420d0591fa.pdf" >https://openreview.net/pdf/edd586d663019327dfd268abca5445420d0591fa.pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/XianguiKang/AdvAD" >https://github.com/XianguiKang/AdvAD<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93401" >https://nips.cc/virtual/2024/poster/93401<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/93401.png) --></li>
<li>idea: 探索非参数扩散以实现不可察觉的对抗性攻击</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106140755.png"
                      alt="20250106140755"
                ></p>
<h3 id="MIA-Member-Inference-Attack"><a href="#MIA-Member-Inference-Attack" class="headerlink" title="MIA (Member Inference Attack)"></a>MIA (Member Inference Attack)</h3><h4 id="CLiD"><a href="#CLiD" class="headerlink" title="CLiD"></a>CLiD</h4><p><em>Membership Inference on Text-to-Image Diffusion Models via Conditional Likelihood Discrepancy</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.14800" >https://arxiv.org/pdf/2405.14800<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/zhaisf/CLiD" >https://github.com/zhaisf/CLiD<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96064" >https://nips.cc/virtual/2024/poster/96064<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/96064.png) --></li>
<li>idea: 基于条件似然差异的T2I扩散模型的成员推断<blockquote>
<p>T2I扩散模型在可控图像生成领域取得了巨大的成功，同时也伴随着隐私泄露和数据版权问题。<br>在这些上下文中，成员推断作为一种潜在的审计方法，用于检测未经授权的数据使用。尽管在扩散模型方面已经做了一些努力，但由于计算开销较大和泛化能力增强，它们不适用于文本到图像的扩散模型。<br>在本文中，我们首先确定了文本到图像扩散模型中的条件过拟合现象，表明这些模型倾向于过拟合给定相应文本的图像的条件分布，而不仅仅是图像的边缘分布。在此基础上，我们推导出一个分析指标，即 <strong>条件似然差异 (CLiD)</strong> ，以执行成员推理，从而降低了估计个体样本记忆的随机性。<br>实验结果表明，该方法在不同的数据分布和数据集尺度上都明显优于以前的方法。此外，我们的方法显示出优越的抵抗过度拟合缓解策略，如早期停止和数据增强。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106140848.png"
                      alt="20250106140848"
                ></p>
<h3 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h3><h4 id="ZoDiac"><a href="#ZoDiac" class="headerlink" title="ZoDiac"></a>ZoDiac</h4><p><em>Attack-Resilient Image Watermarking Using Stable Diffusion</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.04247" >https://arxiv.org/pdf/2401.04247<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/zhanglijun95/ZoDiac" >https://github.com/zhanglijun95/ZoDiac<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/94294" >https://nips.cc/virtual/2024/poster/94294<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>图像水印对于追踪图像来源和证明所有权至关重要。随着稳定扩散等生成模型的出现，这些模型可以创建虚假但逼真的图像，水印对于确保人工创建的图像可可靠识别变得尤为重要。<br>遗憾的是，同样的稳定扩散技术可以去除使用现有方法注入的水印。<br>为了解决这个问题，我们提出了 <strong>ZoDiac</strong>，它 <strong>使用预先训练的稳定扩散模型将水印注入可训练的潜在空间，从而使水印即使在受到攻击时也能在潜在向量中被可靠地检测到</strong>。<br>我们在 MS-COCO、DiffusionDB 和 WikiArt 三个基准测试上对 ZoDiac 进行了评估，发现 ZoDiac 能够抵御最先进的水印攻击，水印检测率超过 98%，误报率低于 6.4%，优于最先进的水印方法。我们假设，扩散模型中的往复式去噪过程可能在面对强攻击时固有地增强水印的鲁棒性，并验证了这一假设。我们的研究表明，稳定扩散是一种很有前景的鲁棒水印方法，甚至能够抵御基于稳定扩散的攻击方法。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/94294.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106140912.png"
                      alt="20250106140912"
                ></p>
<h3 id="Federated-Learning"><a href="#Federated-Learning" class="headerlink" title="Federated Learning"></a>Federated Learning</h3><h4 id="DataStealing"><a href="#DataStealing" class="headerlink" title="DataStealing"></a>DataStealing</h4><p><em>DataStealing: Steal Data from Diffusion Models in Federated Learning with Multiple Trojans</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/pdf?id=792txRlKit" >https://openreview.net/pdf?id=792txRlKit<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/yuangan/DataStealing" >https://github.com/yuangan/DataStealing<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>openreview: <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=792txRlKit" >https://openreview.net/forum?id=792txRlKit<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/96480" >https://nips.cc/virtual/2024/poster/96480<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>联邦学习（FL）通常用于协同训练具有隐私保护的模型。<br>在本文中，我们发现<strong>流行的扩散模型为 FL 引入了新的漏洞</strong>，这带来了严重的隐私威胁。尽管采取了严格的数据管理措施，攻击者仍然可以 <strong>通过多个木马从本地客户端窃取大量隐私数据，这些木马通过多个触发器控制生成行为。</strong> 我们将这项新任务称为 <strong>DataStealing</strong>，并证明攻击者可以基于我们在原始 FL 系统中提出的组合触发器（ComboT）实现目的。<br>然而，基于距离的高级 FL 防御仍然能够根据每个本地更新之间的距离有效地过滤恶意更新。因此，我们提出了一种自适应尺度关键参数（AdaSCP）攻击来绕过防御并将恶意更新无缝地合并到全局模型中。具体而言，AdaSCP 使用扩散模型主要时间步中的梯度来评估参数的重要性。随后，它会自适应地寻求最佳比例因子并在将关键参数更新上传到服务器之前将其放大。因此，恶意更新变得与良性更新相似，使得基于距离的防御难以识别。<br>大量实验表明，使用 FL 训练扩散模型存在泄露数千张图像的风险。此外，这些实验证明了 AdaSCP 在击败高级基于距离的防御方面的有效性。我们希望这项工作能够引起 FL 社区对扩散模型关键隐私安全问题的更多关注。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/96480.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106140956.png"
                      alt="20250106140956"
                ></p>
<h2 id="Defence"><a href="#Defence" class="headerlink" title="Defence"></a>Defence</h2><h3 id="Anti-Adversarial-Prompt"><a href="#Anti-Adversarial-Prompt" class="headerlink" title="Anti-Adversarial Prompt"></a>Anti-Adversarial Prompt</h3><h4 id="GuardT2I"><a href="#GuardT2I" class="headerlink" title="GuardT2I"></a>GuardT2I</h4><p><em>GuardT2I: Defending Text-to-Image Models from Adversarial Prompts</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.01446" >https://arxiv.org/pdf/2403.01446<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/cure-lab/GuardT2I" >https://github.com/cure-lab/GuardT2I<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>model: <a class="link"   target="_blank" rel="noopener" href="https://huggingface.co/YijunYang280/GuardT2I/" >https://huggingface.co/YijunYang280/GuardT2I/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/95982" >https://nips.cc/virtual/2024/poster/95982<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>尽管现有的对策如NSFW分类器或模型微调以<strong>去除不适当的概念</strong>，但T2I模型的最新进展已经引起了人们对其可能被滥用以产生不适当或不适合工作的内容的重大安全担忧。<br>为了应对这一挑战，我们的研究揭示了 <strong>GuardT2I</strong>，这是一个新的调节框架，采用生成方法来 <strong>增强T2I模型对对抗性提示的鲁棒性</strong>。<br>GuardT2I 没有进行二元分类，而是<strong>利用大型语言模型有条件地将T2I模型中的 文本引导embedding 转换为 自然语言</strong>，以实现有效的对对性提示检测，同时不影响模型的固有性能。<br>我们广泛的实验表明，GuardT2I在不同的对抗场景中表现优于领先的商业解决方案，如OpenAI-Moderation和Microsoft Azure Moderator。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/95982.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106141013.png"
                      alt="20250106141013"
                ></p>
<ul>
<li>Stage1: LLM Generation. <ul>
<li>借助 <code>c·LLM</code> ，将T2I模型中的text guidance embedding转换为natural language ( <code>Prompt Interpretation</code> )</li>
<li>将该任务视为一个条件生成任务</li>
<li>合并cross-attention modules 到 pre-trained LLMs，得到一个conditional LLM ( <code>c·LLM</code> )</li>
</ul>
</li>
<li>Stage2: Generation Parsing. 对 <code>Prompt Interpretation</code> 进行双层分析：<ul>
<li><strong>Verbalizer</strong> 检测 <code>Prompt Interpretation</code> 中是否含有NSFW词汇 (简单直接)。<ul>
<li>NSFW词汇是开发者 predefine 的，文中使用了25个常见的NSFW词汇。</li>
</ul>
</li>
<li><strong>Sentence Similarity Checker</strong> 检测生成的 <code>Prompt Interpretation</code> 与 <code>initial prompt</code> 之间的相似度，如果相似度低于某个阈值，则判定是 <strong>潜在恶意(potential malicious)</strong> 的。<ul>
<li>使用了现有的sentence similarity model -&gt; <code>SentenceBERT</code></li>
</ul>
</li>
</ul>
</li>
<li>Decision &amp; Reason:<ul>
<li>根据stage2的结果综合判断，来做出是否reject的决定（中止T2I的推理过程）</li>
<li>GuardT2I于T2I是并行运行的，所以没有额外的性能损失。（只要GuardT2I的运行速度大于T2I的推理速度）<ul>
<li>比Safety Checker快300倍</li>
<li>对模型原有的生成性能影响很小</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>实验部分，也对MMA-Diffusion做了针对性改进，以测试GuardT2I的面对Adaptive Attacks时的能力。</p>
<h3 id="Unlearn"><a href="#Unlearn" class="headerlink" title="Unlearn"></a>Unlearn</h3><h4 id="AdvUnlearn"><a href="#AdvUnlearn" class="headerlink" title="AdvUnlearn"></a>AdvUnlearn</h4><p><em>Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models</em></p>
<ul>
<li>idea: 对抗性训练 + Unlearning</li>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.15234" >https://arxiv.org/pdf/2405.15234<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/OPTML-Group/AdvUnlearn" >https://github.com/OPTML-Group/AdvUnlearn<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>demo: <a class="link"   target="_blank" rel="noopener" href="https://huggingface.co/spaces/Intel/AdvUnlearn" >https://huggingface.co/spaces/Intel/AdvUnlearn<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>Unlearned DM Benchmark: <a class="link"   target="_blank" rel="noopener" href="https://huggingface.co/spaces/Intel/UnlearnDiffAtk-Benchmark" >https://huggingface.co/spaces/Intel/UnlearnDiffAtk-Benchmark<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>HF Model: <a class="link"   target="_blank" rel="noopener" href="https://huggingface.co/OPTML-Group/AdvUnlearn" >https://huggingface.co/OPTML-Group/AdvUnlearn<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/94320" >https://nips.cc/virtual/2024/poster/94320<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型 (DM) 在文本转图像生成方面取得了显著成功，但也存在安全风险，例如可能生成有害内容和侵犯版权。<br>Machine Unlearning（也称为概念擦除）技术已被开发用于应对这些风险。然而，这些技术仍然容易受到对抗性提示攻击，这种攻击可能导致机器学习模型在完成机器学习后重新生成包含本应擦除的概念（例如裸体）的不良图像。<br>本研究旨在通过将 <strong>对抗性训练 (AT) 的原理</strong> 融入Machine Unlearn，增强概念擦除的鲁棒性，从而 <strong>构建了称为 AdvUnlearn 的鲁棒机器学习框架</strong>。<br>然而，有效且高效地实现这一目标并非易事。首先，我们发现直接实施对抗性训练 (AT) 会损害机器学习模型在完成机器学习后的图像生成质量。为了解决这个问题，我们在一个额外的保留集上开发了一个效用保留正则化，以优化 AdvUnlearn 中概念擦除鲁棒性和模型效用之间的权衡。此外，我们认为文本编码器比 UNet 更适合进行鲁棒化，从而确保了去学习的有效性。并且，所获得的文本编码器可以作为各种数据挖掘类型的即插即用型鲁棒去学习器。<br>从实证角度来看，我们进行了大量实验，以证明 AdvUnlearn 在各种数据挖掘去学习场景中的鲁棒性优势，包括裸体、物体和风格概念的擦除。除了鲁棒性之外，AdvUnlearn 还在模型效用之间实现了平衡。<br>据我们所知，这是<strong>第一篇系统地探索通过 AT 进行鲁棒数据挖掘unlearn的研究</strong>，使其有别于现有那些忽视概念擦除鲁棒性的方法。代码可在 <a class="link"   target="_blank" rel="noopener" href="https://github.com/OPTML-Group/AdvUnlearn" >https://github.com/OPTML-Group/AdvUnlearn<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> 获取。警告：本文包含的模型输出可能具有冒犯性。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/94320.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250106141035.png"
                      alt="20250106141035"
                ></p>
<h4 id="Leveraging-Catastrophic-Forgetting"><a href="#Leveraging-Catastrophic-Forgetting" class="headerlink" title="Leveraging Catastrophic Forgetting"></a>Leveraging Catastrophic Forgetting</h4><p><em>Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/pdf?id=pR37AmwbOt" >https://openreview.net/pdf?id=pR37AmwbOt<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>openreview: <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=pR37AmwbOt" >https://openreview.net/forum?id=pR37AmwbOt<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/93554" >https://nips.cc/virtual/2024/poster/93554<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>idea: 利用灾难性遗忘开发安全扩散模型以抵御恶意微调<blockquote>
<p>扩散模型 (DM) 在基于文本提示的图像生成方面表现出显著的能力。人们提出了许多方法来确保这些模型生成安全的图像。<br>早期的方法试图将安全过滤器纳入模型，以减少产生有害图像的风险，但这种外部过滤器本身并不能解除模型的毒性，可以很容易地绕过。因此，考虑到 <strong>模型Unlearn</strong> 和 <strong>数据清洗</strong> 对模型参数的影响，它们是维护模型安全的最基本的方法。然而，即使使用这些方法，恶意的微调仍然会使模型倾向于生成有害或不良的图像。<br>受<strong>灾难性遗忘现象</strong>的启发，我们提出了一种使用 <strong>对比学习</strong> 的训练策略，以 <strong>增加清洁和有害数据分布之间的潜在空间距离，从而保护模型不被微调以生成由于遗忘引起的有害图像。</strong><br>实验结果表明，我们的方法不仅在恶意微调之前保持了清晰的图像生成能力，而且<strong>在恶意微调之后有效地防止了 DM 产生有害的图像。</strong> 我们的方法还可以与其他安全方法相结合，以进一步保持其安全性，防止恶意微调。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://nips.cc/media/PosterPDFs/NeurIPS%202024/93554.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250110161946.png"
                      alt="20250110161946"
                ></p>
<h3 id="Memory-Concept-Location"><a href="#Memory-Concept-Location" class="headerlink" title="Memory&#x2F;Concept Location"></a>Memory&#x2F;Concept Location</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/%E5%9B%BE%E7%89%871.png"
                      alt="图片1"
                ></p>
<h4 id="Finding-NeMo"><a href="#Finding-NeMo" class="headerlink" title="Finding NeMo"></a>Finding NeMo</h4><p><em>Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models</em></p>
<ul>
<li>pdf: <a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.02366" >https://arxiv.org/pdf/2406.02366<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>project page: <a class="link"   target="_blank" rel="noopener" href="https://ml-research.github.io/localizing_memorization_in_diffusion_models/" >https://ml-research.github.io/localizing_memorization_in_diffusion_models/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>code: <a class="link"   target="_blank" rel="noopener" href="https://github.com/ml-research/localizing_memorization_in_diffusion_models" >https://github.com/ml-research/localizing_memorization_in_diffusion_models<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>poster: <a class="link"   target="_blank" rel="noopener" href="https://nips.cc/virtual/2024/poster/94713" >https://nips.cc/virtual/2024/poster/94713<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型（DMs）产生非常详细和高质量的图像。他们的能力来自于对大量数据的广泛训练——这些数据通常是从互联网上抓取的，没有适当的归属或内容创作者的同意。不幸的是，这种做法引起了隐私和知识产权问题，因为DMs可以记住并在推理时再现其潜在的敏感或受版权保护的训练图像。<br>之前的努力通过改变扩散过程的输入来防止这个问题，从而防止DM在推理过程中生成记忆的样本，或者从训练中完全删除记忆的数据。虽然当DMs被开发和部署在一个安全且持续监控的环境中时，这些解决方案是可行的，但它们存在攻击者规避保护措施的风险，并且当DMs本身被公开发布时，这些解决方案是无效的。<br>为了解决这个问题，我们引入了 <strong>NeMo</strong>，这是 <strong>第一种将单个数据样本的记忆定位到DMs的交叉注意层神经元水平的方法。</strong><br>通过我们的实验，我们发现在许多情况下，单个神经元负责记忆特定的训练样本。<strong>通过停用这些记忆神经元，我们可以避免在推理时重复训练数据，增加生成输出的多样性，并减轻私有和版权数据的泄漏。</strong> 通过这种方式，我们的NEMO有助于更负责任的部署DMs。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://nips.cc/media/PosterPDFs/NeurIPS%202024/94713.png"
                     
                ></p>
<ul>
<li>Privacy&#x2F;Copyright issue</li>
<li><strong>Data-Sample-Level Memory</strong></li>
<li>2 kind of Memory: <ul>
<li>Verbatim Memory(VM)：逐字记忆，神经元记忆整个训练样本。</li>
<li>Template Memory(TM)：模板记忆，神经元记忆训练样本的主体构成。</li>
</ul>
</li>
<li>基于：MIA的启发，扩散模型对记忆样本和非记忆样本的相应不同。<ul>
<li>对于记忆的样本，模型预测的initial nosie(根据$X_T$预测的$X_{T-1}$)趋向一致，与seed无关。 &#x3D;&gt; 即预测噪声之于seed的分布是一致且平缓的。</li>
<li>对于非记忆的样本，模型预测的initial nosie更加多样，分布更加陡峭。</li>
</ul>
</li>
<li>方法：随机停用（先随机停用layer筛选出可能的layer，然后对可能的layer随机停用神经元，筛选出可能的memory neuron）</li>
<li>most training data samples are memorized by just <strong>a few or even a single neuron</strong>.</li>
</ul>
<h4 id="P-ESD-P-AC-SGAW-Wrokshop"><a href="#P-ESD-P-AC-SGAW-Wrokshop" class="headerlink" title="P-ESD&#x2F;P-AC [SGAW Wrokshop]"></a>P-ESD&#x2F;P-AC [SGAW Wrokshop]</h4><p><em>Pruning for Robust Concept Erasing in Diffusion Models</em></p>
<ul>
<li><p>pdf: <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/pdf?id=jD1eWpUMOf" >https://openreview.net/pdf?id=jD1eWpUMOf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
<li><p>poster in workshop: <a class="link"   target="_blank" rel="noopener" href="https://neurips.cc/virtual/2024/106210" >https://neurips.cc/virtual/2024/106210<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
<li><p>openreview(8-5-8): <a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=jD1eWpUMOf" >https://openreview.net/forum?id=jD1eWpUMOf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<blockquote>
<p>我们引入了一个简单而有效的 <strong>基于剪枝的概念擦除框架</strong>。<br>通过 <strong>将概念擦除和剪枝集成到一个目标中</strong> ，我们的方法有效地消除了模型中的概念知识，同时切断了可能重新激活概念相关隐藏状态的路径，确保了 <strong>对抗性提示的稳健性</strong>。<br>实验结果表明，我们的模型对对抗性攻击的抵御能力得到了显著增强。与现有的概念擦除方法相比，我们的方法在 NSFW 内容和艺术作品风格的擦除方面取得了约 30% 的提升。</p>
</blockquote>
</li>
<li><p>NSFW&#x2F;Copyright issue</p>
</li>
<li><p><strong>Concept-Level Memory</strong></p>
</li>
<li><p>Background: fine-tuning-based erasing is vulnerable to adversarial attacks !</p>
<ul>
<li>Existing erasing methods fine-tune parameters to “deactivate” such neurons to achieve removal in training data. However, these neurons might be “reactivated” when inputs are cleverly designed（对擦除概念很重要的神经元对明确设计的对抗性提示很敏 感。因此，它们可以被”重新激活”以重新生成要删除的概念。）</li>
</ul>
</li>
<li><p>Hypothesis: the generation of a specific concept is correlated with <strong>a subset of neurons in diffusion models</strong>, which we refer to as concept neurons in this paper</p>
<ul>
<li>the generation of a specific concept is correlated with a subset of neurons in diffusion models, which we refer to as <code>concept neurons</code> in this paper</li>
</ul>
</li>
<li><p>基于：神经元的激活程度与概念的生成相关。</p>
</li>
<li><p>设计了两种方法：</p>
<ul>
<li>NP-ESD （直接剪除变化最大的top1神经元；擦除效果差，干扰大）（只能说明这些是构成该concept的重要神经元，但是不能说明是区别于其他concept的关键神经元）</li>
<li>P-ESD （hard掩码优化；擦除更好，干扰更小）</li>
</ul>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250110160709.png"
                      alt="20250110160709"
                ></p>
<h1 id="CVPR-2025"><a href="#CVPR-2025" class="headerlink" title="CVPR 2025"></a>CVPR 2025</h1><p>Submission Deadline: 2024.11.15</p>
<p>Official Site:</p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/papers.html" >CVPR 2025 Papers<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<p>WeChat Article:</p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/PWHZr6D4oQiZ-6c-gHA3tw" >我用Gemini处理了28篇CVPR 25文章——看看LLM怎么解读CVPR最新T2I动向<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h2 id="Attack-1"><a href="#Attack-1" class="headerlink" title="Attack"></a>Attack</h2><h3 id="Adversarial-Attack-1"><a href="#Adversarial-Attack-1" class="headerlink" title="Adversarial Attack"></a>Adversarial Attack</h3><h4 id="FastProtect"><a href="#FastProtect" class="headerlink" title="FastProtect"></a>FastProtect</h4><p><em>Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models</em></p>
<ul>
<li>TL;DR: 针对DMs的几乎零开销的实时的图像保护方法</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Ahn_Nearly_Zero-Cost_Protection_Against_Mimicry_by_Personalized_Diffusion_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33351" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://youtu.be/IV9hoKMgX1Q" >video<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型的最新进展彻底改变了图像生成，但也带来了滥用的风险，例如复制艺术品或生成深度伪造作品。<br>现有的图像保护方法虽然有效，但难以在保护效果(protection performance)、不可见性(invisibility)和延迟(inference time)之间取得平衡，从而限制了实际应用。<br>我们引入 <strong>扰动预训练</strong> 来降低延迟，并提出了一种 <strong>混合扰动方法</strong> ，该方法可以动态地适应输入图像，从而最大限度地减少性能下降。<br>我们新颖的训练策略 <strong>计算跨多个 VAE 特征空间的保护损失</strong> ，而推理阶段的 <strong>自适应定向保护则增强了鲁棒性和隐形性</strong>。<br>实验表明，该方法具有相当的protection performance，并且invisibility得到提升，inference time也显著缩短。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014135435022.png"
                     
                ></p>
<h4 id="I2VGuard"><a href="#I2VGuard" class="headerlink" title="I2VGuard"></a>I2VGuard</h4><p><em>I2VGuard: Safeguarding Images against Misuse in Diffusion-based Image-to-Video Models</em></p>
<ul>
<li>TL;DR: 针对T2V DMs, 在空间和时间两个维度，利用对抗攻击保护图像（降低生成Video的质量）</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Gui_I2VGuard_Safeguarding_Images_against_Misuse_in_Diffusion-based_Image-to-Video_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/35251" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>图像转视频生成领域的最新进展使得静态图像的动画化成为可能，并提供了像素级的可控性。虽然这些模型在将单幅图像转换为生动动态视频方面拥有巨大潜力，但它们也存在滥用风险，可能影响隐私、安全和版权保护。<br>本文提出了一种新颖的方法，<strong>对图像施加难以察觉的扰动来降低生成视频的质量，从而保护图像免遭白盒图像转视频扩散模型的滥用。</strong> 具体而言，我们将该方法作为一种对抗性攻击，结合了空间、时间和扩散攻击模块。<strong>空间攻击</strong> 将图像特征从其原始分布转移到质量较低的目标分布，从而降低了视觉保真度。<strong>时间攻击</strong> 通过干扰引导运动生成的时间注意力图来破坏连贯运动。<br>为了增强我们的方法在不同模型中的鲁棒性，我们进一步提出了 <strong>一个利用对比损失的扩散攻击模块</strong> 。我们的方法可以轻松地与主流的基于扩散的 I2V 模型集成。<br>在 SVD、CogVideoX 和 ControlNeXt 上进行的大量实验表明，我们的方法会显著降低生成质量（包括视觉清晰度和运动一致性），同时仅会在图像中引入极少的伪影。据我们所知，我们是首个出于安全目的探索T2V对抗攻击的团队。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014190103177.png"
                     
                ></p>
<h3 id="Data-Poisoning-Attack"><a href="#Data-Poisoning-Attack" class="headerlink" title="Data Poisoning Attack"></a>Data Poisoning Attack</h3><h4 id="Silent-Branding-Attack"><a href="#Silent-Branding-Attack" class="headerlink" title="Silent Branding Attack"></a>Silent Branding Attack</h4><p><em>Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models</em></p>
<ul>
<li>TL;DR: 一种数据投毒方法，让模型生成的图片包含 specific brand logo，并且不需要 text trigger。</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Jang_Silent_Branding_Attack_Trigger-free_Data_Poisoning_Attack_on_Text-to-Image_Diffusion_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/35225" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://silent-branding.github.io/" >webpage<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本到图像的扩散模型在从文本提示生成高质量内容方面取得了显著成功。然而，由于它们依赖于公开数据，并且为了进行微调而共享数据，这些模型尤其容易受到数据中毒攻击。<br>本文提出了一种名为 <strong>“静默品牌攻击”（Silent Branding Attack）</strong> 的新型数据中毒方法，它能够 <strong>操纵T2I DMs，生成包含特定品牌标识或符号的图像，而无需任何文本触发。</strong><br>我们发现，<strong>当某些视觉模式在训练数据中重复出现时，即使没有提示，模型也能学会在输出中自然地重现这些模式</strong>。利用这一特性，我们开发了一种 <strong>自动化数据中毒算法，该算法可以不显眼地将标识注入原始图像中，确保它们自然融合且不被检测到。</strong> 在此中毒数据集上训练的模型能够生成包含标识的图像，而不会降低图像质量或文本对齐。<br>我们在大规模高质量图像数据集和风格个性化数据集上，通过两种实际设置实验验证了我们的静默品牌攻击，即使没有特定的文本触发，也能获得很高的成功率。人工评估和包括徽标检测在内的定量指标表明，我们的方法可以隐秘地嵌入徽标。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014151718634.png"
                     
                ></p>
<h3 id="MIA"><a href="#MIA" class="headerlink" title="MIA"></a>MIA</h3><h4 id="CDI-Copyrighted-Data-Identification"><a href="#CDI-Copyrighted-Data-Identification" class="headerlink" title="CDI (Copyrighted Data Identification)"></a>CDI (Copyrighted Data Identification)</h4><p><em>CDI: Copyrighted Data Identification in Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Dubinski_CDI_Copyrighted_Data_Identification_in_Diffusion_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34687" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://youtu.be/eWr2KGhx0Tw" >video<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型 (DMs) 的训练得益于海量且多样化的数据集。由于这些数据通常是未经数据所有者许可从互联网上抓取的，这引发了人们对版权和知识产权保护的担忧。<br>虽然对于由 DMs 在推理时完美重建的训练样本，数据的（非法）使用很容易被检测到，但当可疑 DMs 的输出不是近似副本时，数据所有者很难验证他们的数据是否用于训练。从概念上讲，成员推断攻击 (MIA) 可以检测给定数据点是否在训练期间被使用，它是解决这一挑战的合适工具。然而，我们证明现有的 MIA 不足以在大型、最先进的 DMs 中可靠地确定单个图像的成员资格。<br>为了克服这一限制，我们提出了 <strong>CDI，一个供数据所有者识别其数据集是否用于训练给定 DMs 的框架。</strong><br>CDI 依赖于 <strong>数据集推断技术</strong>，即 CDI 并非使用来自单个数据点的成员资格信号，而是利用了这样一个事实：<strong>大多数数据所有者（例如图片库提供商、视觉媒体公司，甚至个人艺术家）都拥有包含多个公开data points的数据集，这些data points可能全部用于训练特定的 DMs 。</strong><br>通过选择性地聚合来自现有 MIAs 的信号，并使用新的人工方法提取这些数据集的特征，将其输入评分模型，并进行严格的统计测试，CDI 允许数据所有者 <strong>仅使用 70 个 data points，以超过 99% 的置信度</strong> 识别其数据是否被用于训练特定的数据挖掘模型 (DM)。因此，CDI 是数据所有者对其版权数据被非法使用进行索赔的有力工具。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014143944627.png"
                     
                ></p>
<h3 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h3><h4 id="IBI-Implicit-Bias-Injection-Attacks"><a href="#IBI-Implicit-Bias-Injection-Attacks" class="headerlink" title="IBI (Implicit Bias Injection Attacks)"></a>IBI (Implicit Bias Injection Attacks)</h4><p><em>Implicit Bias Injection Attacks against Text-to-Image Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Implicit_Bias_Injection_Attacks_against_Text-to-Image_Diffusion_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34474" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本转图像扩散模型 (T2I DMs) 的普及使得人工智能生成的图像在日常生活中日益常见。然而，存在偏见的 T2I 模型可能会生成具有特定倾向的内容，从而可能影响人们的感知。故意利用这些偏见可能会向公众传递误导性信息。<br>目前对偏见的研究主要针对具有可识别视觉模式的显性偏见，例如肤色和性别。<br>本文介绍了 <strong>一种新型的隐性偏见，它缺乏显性视觉特征，但可以在不同的语义语境中以多种方式表现出来。</strong> 这种微妙且多变的特性使得这种偏见难以检测、易于传播，并且能够适应各种场景。<br>我们进一步提出了一个针对 T2I 扩散模型的 <strong>隐性偏见注入攻击框架 (IBI-Attacks)，该框架通过在提示嵌入空间中预先计算一个通用的偏见方向，并根据不同的输入自适应地调整它。</strong> 我们的攻击模块可以无缝集成到预训练的扩散模型中，即插即用，无需直接操作用户输入或重新训练模型。<br>大量实验验证了我们的方案在保留原始语义的同时，通过微妙而多样的修改引入偏差的有效性。我们的攻击在各种场景中的强大隐蔽性和可转移性进一步强调了我们的方法的重要性。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251008004029738.png"
                     
                ></p>
<h3 id="Watermarks"><a href="#Watermarks" class="headerlink" title="Watermarks"></a>Watermarks</h3><h4 id="Black-Box-Forgery-Attacks-on-Semantic-Watermarks"><a href="#Black-Box-Forgery-Attacks-on-Semantic-Watermarks" class="headerlink" title="Black-Box Forgery Attacks on Semantic Watermarks"></a>Black-Box Forgery Attacks on Semantic Watermarks</h4><p><em>Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models</em></p>
<ul>
<li>TL;DR: 黑盒条件下 伪造 语义水印</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34820" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   href="ttps://github.com/and-mill/semantic-forgery" >code<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>将水印集成到潜在扩散模型 (LDMs) 的生成过程中，可以简化生成内容的检测和归因。<br>语义水印，例如树形年轮(Tree-Rings)和高斯阴影(Gaussian Shading)，代表了一类新颖的水印技术，易于实现，并且对各种扰动具有高度的鲁棒性。<br>然而，我们的工作揭示了 <strong>语义水印的一个根本安全漏洞</strong> 。我们表明，<strong>攻击者可以利用不相关的模型，即使使用不同的潜在空间和架构（UNet 与 DiT），也能执行强大且逼真的伪造攻击</strong>。<br>具体来说，我们设计了 <strong>两种水印伪造攻击</strong> 。第一种攻击通过 <strong>在不相关的 LDMs 中操纵任意图像的潜在表示，使其更接近带水印图像的潜在表示(Imprint-F)</strong> ，从而将目标水印嵌入真实图像中。我们还表明，该技术可用于去除水印(Imprint-R)。第二种攻击通过 **反转带水印图像并使用任意提示重新生成它，来生成带有目标水印的新图像(Reprompt)**。<br>两种攻击都只需要一张带有目标水印的参考图像。总而言之，我们的研究结果质疑了语义水印的适用性，因为攻击者在现实条件下很容易伪造或移除这些水印。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014144622223.png"
                     
                ></p>
<h2 id="Defence-1"><a href="#Defence-1" class="headerlink" title="Defence"></a>Defence</h2><h3 id="Guidence"><a href="#Guidence" class="headerlink" title="Guidence"></a>Guidence</h3><h4 id="DAG-Detect-and-Guide"><a href="#DAG-Detect-and-Guide" class="headerlink" title="DAG (Detect-and-Guide)"></a>DAG (Detect-and-Guide)</h4><p><em>Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Detect-and-Guide_Self-regulation_of_Diffusion_Models_for_Safe_Text-to-Image_Generation_via_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/32690" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本到图像的扩散模型在合成任务中取得了最先进的结果； 然而，人们越来越担心它们可能被滥用来创建有害内容。为了减轻这些风险，人们开发了事后模型干预技术，例如概念遗忘和安全指导。<br>然而，微调模型权重或调整扩散模型的隐藏状态以一种不可解释的方式进行，使得不清楚中间变量的哪一部分负责不安全的生成。当从复杂的多概念提示中删除有害概念时，这些干预措施会严重影响采样轨迹，从而阻碍了它们在现实世界中的实际使用。尽管它们在单一概念提示上有效，但当前的方法仍然面临着挑战，因为它们很难在不破坏良性概念语义的情况下精确删除有害概念。<br>在这项工作中，我们提出了 <strong>检测和引导（DAG）</strong> 安全生成框架， <strong>利用扩散模型的内部知识在采样过程中进行自我诊断和细粒度的自我调节。</strong><br>DAG首先使用优化标记的细化交叉注意力图从噪声潜伏中检测有害概念，然后应用具有自适应强度和编辑区域的安全引导来否定不安全生成。优化只需要小型注释数据集，可以提供具有普遍性和概念特异性的精确检测图。 此外，<strong>DAG不需要对扩散模型进行微调</strong>，因此不会对其生成多样性造成损失。<br>擦除色情内容的实验表明，DAG 实现了最先进的安全生成性能，在多概念现实世界提示上平衡了危害缓解和文本跟踪性能。</p>
</blockquote>
</li>
</ul>
<h4 id="Concept-Replacer"><a href="#Concept-Replacer" class="headerlink" title="Concept Replacer"></a>Concept Replacer</h4><p>Concept Replacer: Replacing Sensitive Concepts in Diffusion Models via Precision Localization</p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Concept_Replacer_Replacing_Sensitive_Concepts_in_Diffusion_Models_via_Precision_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33513" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>随着大规模扩散模型的不断发展，它们在生成高质量图像方面表现出色，但同时也常常会生成一些不受欢迎的内容，例如色情或暴力内容。<br>现有的概念移除方法通常会引导图像生成过程，但可能会无意中修改不相关的区域，从而导致与原始模型不一致。<br>我们提出了一种新的扩散模型中的<strong>目标概念替换方法</strong>，能够在不影响非目标区域的情况下移除特定概念。<br>我们的方法引入了 <strong>一个专用的概念定位器，用于在去噪过程中精确识别目标概念</strong> ，该定位器采用少样本学习进行训练，只需要极少的标记数据。在已识别的区域内，我们引入了一个无需训练的 <strong>双提示交叉注意力 (DPCA) 模块</strong> 来替换目标概念，确保对周围内容的干扰最小。<br>我们对我们的方法进行了 <strong>概念定位精度</strong> 和 <strong>替换效率</strong> 的评估。实验结果表明，我们的方法在目标概念定位方面取得了卓越的精度，并在对非目标区域影响最小的情况下进行了连贯的概念替换，优于现有方法。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014133757225.png"
                     
                ></p>
<h3 id="Unlern"><a href="#Unlern" class="headerlink" title="Unlern"></a>Unlern</h3><h4 id="EraseDiff"><a href="#EraseDiff" class="headerlink" title="EraseDiff"></a>EraseDiff</h4><p><em>Erasing Undesirable Influence in Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Erasing_Undesirable_Influence_in_Diffusion_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33939" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>note: 考虑到“擦除$L_f$”和“保留$L_r$”两个目标，对vanilla MOO（多目标优化）改进，变成contrained optimization problem（有natutral first-order solution）。解决了优化梯度冲突问题。<blockquote>
<p>扩散模型在生成高质量图像方面非常有效，但也存在风险，例如无意中生成 NSFW（不适合工作）内容。<br>尽管已经提出了各种技术来减轻扩散模型中的不良影响，同时保持整体性能，但在这些目标之间取得平衡仍然具有挑战性。<br>在这项工作中，我们引入了 EraseDiff，这是一种旨在 <strong>保留扩散模型对保留数据的效用，同时删除与要遗忘的数据相关的不需要的信息的算法。</strong><br>我们的方法使用值函数将此任务表述为约束优化问题，从而产生用于解决优化问题的自然一阶算法。通过改变生成过程以偏离地面实况去噪轨迹，我们更新保存参数，同时控制约束减少以确保有效擦除，从而达到最佳权衡。<br>大量的实验和与最先进算法的彻底比较表明，EraseDiff 有效地保持了模型的效用、功效和效率。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251008005106312.png"
                     
                ></p>
<h4 id="Localized-Concept-Erasure-GLoCE"><a href="#Localized-Concept-Erasure-GLoCE" class="headerlink" title="Localized Concept Erasure (GLoCE)"></a>Localized Concept Erasure (GLoCE)</h4><p><em>Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_Localized_Concept_Erasure_for_Text-to-Image_Diffusion_Models_Using_Training-Free_Gated_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34742" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>基于微调的概念擦除已显示出良好的效果，它通过移除目标概念并保留剩余概念，防止文本到图像的扩散模型生成有害内容。为了在概念擦除后保持扩散模型的生成能力，需要仅移除图像中局部出现的目标概念所在的图像区域，而保留其他区域。<br>然而，现有技术通常会为了擦除出现在特定区域的局部目标概念而牺牲其他图像区域的保真度，从而降低图像生成的整体性能。<br>为了解决这些限制，我们首先引入了一个称为 <strong>Localized Concept Erasure</strong> 的框架，<strong>该框架允许仅删除图像中包含目标概念的特定区域，同时保留其他区域</strong>。<br>作为局部概念擦除的解决方案，我们提出了一种 <strong>training-free</strong> 的方法，称为 <strong>Gated Low-rank adaptation for Concept Erasure (GLoCE)</strong> ，该方法在扩散模型中注入了一个轻量级模块。GLoCE 由低秩矩阵和一个简单的门组成，仅由几个无需训练的概念生成步骤决定。通过将 GLoCE 直接应用于图像嵌入，并设计仅针对目标概念激活的门控，GLoCE 可以选择性地仅移除目标概念的区域，即使目标概念和剩余概念共存于图像中。<br>大量实验表明，GLoCE 不仅在擦除局部目标概念后提高了图像与文本提示的保真度，而且在有效性、特异性和稳健性方面也大幅超越现有技术，并且可以扩展到大规模概念擦除。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251013153220944.png"
                     
                ></p>
<h4 id="ACE-Anti-editing-Concept-Erasure"><a href="#ACE-Anti-editing-Concept-Erasure" class="headerlink" title="ACE (Anti-editing Concept Erasure)"></a>ACE (Anti-editing Concept Erasure)</h4><p><em>ACE: Anti-Editing Concept Erasure in Text-to-Image Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_ACE_Anti-Editing_Concept_Erasure_in_Text-to-Image_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33574" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/120L020904/ACE" >github<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本到图像传播模型的最新进展极大地促进了高质量图像的生成，但也引发了人们对非法创建有害内容（例如受版权保护的图像）的担忧。<br>现有的概念擦除方法在防止提示中产生被擦除的概念方面取得了优异的效果，但在防止不必要的编辑方面通常表现不佳。<br>为了解决这个问题，我们提出了 <strong>一种 Anti-editing 的 Concept Erasure (ACE) 方法，它不仅在生成过程中擦除目标概念，还在编辑过程中将其过滤掉。</strong><br>具体而言，我们建议 <strong>在条件和非条件噪声预测中注入擦除指导</strong> ，使模型能够有效地防止在编辑和生成过程中产生擦除概念。此外， <strong>在训练过程中引入随机校正指导</strong> ，以解决无关概念的擦除问题。<br>我们使用代表性编辑方法（例如 <code>LEDITS++</code> 和 <code>MasaCtrl</code>）进行了擦除编辑实验，以擦除 IP 字符，结果表明，我们的 ACE 能够有效地在两种编辑类型中过滤掉目标概念。进一步的实验（删除显性概念和艺术风格）进一步证明了我们的 ACE 比最先进的方法表现更佳。我们的代码将公开发布。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251013153539968.png"
                     
                ></p>
<h4 id="STEREO-Search-Thoroughly-Enough-Robustly-Erase-Once"><a href="#STEREO-Search-Thoroughly-Enough-Robustly-Erase-Once" class="headerlink" title="STEREO (Search Thoroughly Enough, Robustly Erase Once)"></a>STEREO (Search Thoroughly Enough, Robustly Erase Once)</h4><p><em>STEREO: A Two-Stage Framework for Adversarially Robust Concept Erasing from Text-to-Image Diffusion Models</em> <code>Highlight</code></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/32792" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>大规模文本转图像扩散 (T2ID) 模型的快速普及引发了人们对其可能被滥用生成有害内容的严重担忧。尽管已经提出了许多从 T2ID 模型中擦除不良概念的方法，但它们常常会给人一种虚假的安全感，因为概念擦除模型 (CEM) 很容易被对抗性攻击欺骗，从而生成被擦除的概念。<br>尽管最近出现了一些 <strong>基于对抗性训练的鲁棒概念擦除方法</strong>，但它们为了获得鲁棒性，会牺牲实用性（良性概念的生成质量），并且&#x2F;或者仍然容易受到高级嵌入空间攻击。这些局限性源于鲁棒 CEM 未能彻底搜索嵌入空间中的“盲点(blind spots)”。<br>为了弥补这一缺陷，我们提出了 <strong>STEREO，这是一个新颖的两阶段框架</strong>，它将对抗性训练作为鲁棒概念擦除的第一步，而非唯一步骤。<br>在第一阶段，<strong>Search Thoroughly Enough (STE)：使用对抗性训练作为漏洞识别机制</strong>，以进行足够彻底的搜索；在第二阶段，<strong>Robustly Erase Once (REO)：引入了一个基于锚点概念的(anchor-concept-based)组合目标，以鲁棒的方式一次性擦除目标概念，同时尽量减少模型效用的下降。</strong><br>我们将 STEREO 与 7 种最先进的概念擦除方法进行了对比，证明了其在抵御白盒、黑盒和高级嵌入空间攻击方面具有增强的鲁棒性，并且能够在很大程度上保留效用。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251013162042504.png"
                     
                ></p>
<h4 id="RIIDL-Responsible-Interpretable-Intermediate-Diffusion-Latents"><a href="#RIIDL-Responsible-Interpretable-Intermediate-Diffusion-Latents" class="headerlink" title="RIIDL (Responsible Interpretable Intermediate Diffusion Latents)"></a>RIIDL (Responsible Interpretable Intermediate Diffusion Latents)</h4><p><em>Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Azam_Plug-and-Play_Interpretable_Responsible_Text-to-Image_Generation_via_Dual-Space_Multi-facet_Concept_Control_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33451" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://basim-azam.github.io/responsiblediffusion/" >webpage<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>围绕文本转图像 (T2I) 模型的伦理问题要求对生成内容进行全面控制。现有的针对负责任的 T2I 模型的这些问题的技术旨在使生成的内容公平安全（非暴力&#x2F;明确）。然而，这些方法仍然局限于单独处理责任概念的各个方面，同时也缺乏可解释性。此外，它们通常需要修改原始模型，这会影响模型性能。<br>在本文中，我们提出了一种独特的技术，通过同时考虑广泛的概念来实现负责任的 T2I 生成，并且以可扩展的方式实现公平安全的内容生成。关键思想是 使用 <strong>外部的即插即用(plug-and-play)机制</strong> 蒸馏 target T2I pipeline ，该机制根据 target T2I pipeline 学习 the desired concepts 的 <strong>可解释的合成的责任空间(an interpretable composite responsible space)。</strong><br>我们使用 <strong>知识蒸馏(knowledge distillation) 和 概念白化(concept whitening)</strong> 来实现这一点。在推理时，学习到的空间用于调节生成内容。典型的 T2I 管道为我们的方法提供了两个插件点，即：<strong>文本嵌入空间</strong> 和 <strong>扩散模型潜在空间</strong>。<br>我们针对这两个方面开发了模块，并通过一系列强大的结果证明了我们方法的有效性。我们的代码和模型将在论文被接受后公开。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251013161422499.png"
                     
                ></p>
<h4 id="AdaVD-Adaptive-Vaule-Decomposer"><a href="#AdaVD-Adaptive-Vaule-Decomposer" class="headerlink" title="AdaVD (Adaptive Vaule Decomposer)"></a>AdaVD (Adaptive Vaule Decomposer)</h4><p><em>Precise, Fast, and Low-cost Concept Erasure in Value Space: Orthogonal Complement Matters</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Precise_Fast_and_Low-cost_Concept_Erasure_in_Value_Space__CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34263" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型成功实现了文本到图像的生成，这迫切需要以精确、及时且低成本的方式从预训练模型中擦除不需要的概念，例如版权、冒犯性和不安全的概念。概念擦除的双重需求要求在生成过程中精确删除目标概念（即erasure efficacy），同时对非目标内容生成的影响最小（即prior preservation）。<br>现有方法要么计算成本高昂，要么在维持擦除效率和先验保留之间的有效平衡方面面临挑战。<br>为了改进，我们提出了 <strong>一种精确、快速且低成本的概念擦除方法，称为 Adaptive Vaule Decomposer (AdaVD, 自适应的Value分解器)，它无需训练。</strong><br>该方法基于经典的 <strong>线性代数正交补运算，在扩散模型UNet中每个交叉注意层的值空间中实现。</strong> 设计了<strong>一种有效的移位因子，用于自适应地控制擦除强度，在不牺牲擦除效率的情况下增强先验保留。</strong><br>大量实验结果表明，所提出的 AdaVD 在单概念和多概念擦除方面均有效，与第二佳方法相比，先验保存率提高了 2 到 10 倍，同时，与基于训练和无需训练的现有技术相比，均达到了最佳或接近最佳的擦除效率。AdaVD 支持一系列扩散模型和下游图像生成任务，其代码即将公开。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251013163517981.png"
                     
                ></p>
<h4 id="FADE-Fine-grained-Attenuation-for-Diffusion-Erasure"><a href="#FADE-Fine-grained-Attenuation-for-Diffusion-Erasure" class="headerlink" title="FADE (Fine-grained Attenuation for Diffusion Erasure)"></a>FADE (Fine-grained Attenuation for Diffusion Erasure)</h4><p><em>Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models</em></p>
<ul>
<li>TL;DR: 减少概念擦除时 对 邻接&#x2F;相关概念 的影响</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Thakral_Fine-Grained_Erasure_in_Text-to-Image_Diffusion-based_Foundation_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33583" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>现有的文本转图像生成模型中的去学习算法在移除特定目标概念时，往往无法保留语义相关概念的知识——这一挑战被称为 <strong>adjacency(邻接)</strong> 。<br>为了解决这个问题，我们提出了<strong>FADE（Fine-grained Attenuation for Diffusion Erasure，细粒度衰减扩散擦除）</strong>，在扩散模型中引入了 <strong>adjacency-aware unlearning</strong> 。<br>FADE 包含两个组件：**(1) Concept Lattice（概念格）<strong>，用于识别相关概念的邻接集；</strong>(2) Mesh Modules（网格模块）**，采用结构化的擦除、邻接和引导损失组件组合。这些组件能够 <strong>精确擦除目标概念，同时保持相关和不相关概念之间的保真度。</strong><br>通过在 Stanford Dogs、Oxford Flowers、CUB、I2P、Imagenette 和 ImageNet-1k 等数据集上进行评估，FADE 有效地删除了目标概念，对相关概念的影响最小，与最先进的方法相比，保留性能至少提高了 12%。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014145855328.png"
                     
                ></p>
<h4 id="TIU-The-Illusion-of-Unlearning"><a href="#TIU-The-Illusion-of-Unlearning" class="headerlink" title="TIU (The Illusion of Unlearning)"></a>TIU (The Illusion of Unlearning)</h4><p><em>The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text-to-Image Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/George_The_Illusion_of_Unlearning_The_Unstable_Nature_of_Machine_Unlearning_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33746" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/NGK2110/TIU" >github<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文生图模型，例如Stable Diffusion、DALL·E 和 Midjourney，近年来人气飙升。然而，这些模型基于海量数据进行训练，其中可能包含未经许可使用的私密、露骨或受版权保护的内容，这引发了严重的法律和伦理问题。鉴于近期旨在保护个人数据隐私的法规，旨在从模型中移除特定概念的Machine Unlearn(MU)方法激增。<br>然而，我们发现这些Unlearn技术存在一个关键缺陷：<strong>即使使用一般或不相关的提示，当模型进行微调时，已学习的概念仍会重新出现。</strong><br>本文通过广泛的研究，首次揭示了 <strong>文生图DMs中现有Unlearn方法的不稳定性。</strong><br>我们提出了<strong>一个包含若干指标的框架</strong>，用于分析现有Unlearn方法的稳定性。<br>此外，本文对基于映射的Unlearn方法不稳定性的原因进行了初步探讨，这些见解可以指导未来研究更稳健的Unlearn技术。提供了用于实施所提框架的匿名代码。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251013155034971.png"
                     
                ></p>
<h4 id="Six-CD-Benchmark"><a href="#Six-CD-Benchmark" class="headerlink" title="Six-CD Benchmark"></a>Six-CD Benchmark</h4><p><em>Six-CD: Benchmarking Concept Removals for Text-to-image Diffusion Models</em></p>
<ul>
<li>TL;DR: 概念擦除基准测试数据集&#x2F;指标</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Ren_Six-CD_Benchmarking_Concept_Removals_for_Text-to-image_Diffusion_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34826" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本转图像 (T2I) 扩散模型在生成与文本提示紧密对应的图像方面展现出卓越的能力。然而，T2I 扩散模型的进步也带来了巨大的风险，因为这些模型可能被用于恶意目的，例如生成包含暴力或裸体的图像，或在不恰当的语境中创建未经授权的公众人物肖像。<br>为了降低这些风险，一些概念移除方法被提出。这些方法旨在修改扩散模型，以防止生成恶意和不受欢迎的概念。尽管做出了这些努力，现有研究仍面临一些挑战：(1) 缺乏对综合数据集的一致性比较；(2) 有害和裸体概念中的提示无效；(3) 忽视了对包含恶意概念的提示中生成良性部分的能力的评估。<br>为了弥补这些不足，我们建议通过引入一个 <strong>新数据集 Six-CD</strong> 以及一个 <strong>全新的评估指标</strong> 来对概念移除方法进行基准测试。<br>在该基准测试中，我们对概念移除进行了全面的评估，实验观察和讨论为该领域提供了宝贵的见解。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014145656219.png"
                     
                ><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014145628765.png"
                     
                ></p>
<h3 id="Bias-1"><a href="#Bias-1" class="headerlink" title="Bias"></a>Bias</h3><h4 id="MPR-Multi-group-Proportional-Representation"><a href="#MPR-Multi-group-Proportional-Representation" class="headerlink" title="MPR (Multi-group Proportional Representation)"></a>MPR (Multi-group Proportional Representation)</h4><p><em>Multi-Group Proportional Representations for Text-to-Image Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34035" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本转图像生成模型可以根据文本描述创建生动逼真的图像。随着这些模型的普及，它们也暴露出新的担忧，即它们能否代表不同的人口群体、传播刻板印象以及抹去少数群体的形象。<br>尽管人们越来越关注人工智能 (AI) 的“安全”和“负责任”设计，但目前尚无成熟的方法来系统地测量和控制大型图像生成模型中的表征损害。<br>本文介绍了一个 <strong>用于测量T2IDMs生成的图像中 交叉群体表征 的全新框架。</strong><br>我们提出了一种新的 <strong>多群体均衡的表征 (MPR) 指标</strong> 的应用，以严格评估图像生成中的表征损害，并 <strong>开发了一种算法来优化生成模型以达到该表征指标</strong>。MPR 评估生成模型生成的图像中 <strong>给定人群群体表征统计数据的最坏情况偏差</strong>，从而允许根据用户需求进行灵活且针对特定情境的测量。<br>通过实验，我们证明 MPR 可以 <strong>有效地测量多个交叉群体的代表性统计数据</strong>，并且当用作训练目标时，可以 <strong>引导模型在保持生成质量的同时，实现跨人口群体的更平衡的生成。</strong></p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251013164853941.png"
                     
                ></p>
<h4 id="Rethinking-Training-for-De-biasing-Text-to-Image-Generation"><a href="#Rethinking-Training-for-De-biasing-Text-to-Image-Generation" class="headerlink" title="Rethinking Training for De-biasing Text-to-Image Generation"></a>Rethinking Training for De-biasing Text-to-Image Generation</h4><p><em>Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Kim_Rethinking_Training_for_De-biasing_Text-to-Image_Generation_Unlocking_the_Potential_of_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/34534" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本转图像模型（例如稳定扩散）的最新进展显示出明显的人口统计学偏差。<br>现有的去偏差技术严重依赖于额外的训练，这会带来高昂的计算成本，并可能损害核心图像生成功能。这阻碍了它们在实际应用中的广泛应用。<br>在本文中，我们探索了稳定扩散在 <strong>无需额外训练的情况下降低偏差的潜力</strong> ，这一潜力被人们忽视。<br>通过分析，我们发现与少数族裔属性相关的初始噪声构成了稳定扩散中“少数族裔区域”降低偏差的机会。为了释放这一潜力，我们提出了 <strong>一种名为 “弱引导(weak guidance)” 的新型去偏差方法</strong>，该方法经过精心设计，可以 <strong>在不损害语义完整性的情况下将随机噪声引导至少数族裔区域。</strong><br>通过对不同版本稳定扩散的分析和实验，我们证明了我们提出的方法能够在无需额外训练的情况下有效降低偏差，既实现了效率，又保留了核心图像生成功能。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014135522383.png"
                     
                ></p>
<h3 id="Watermarks-1"><a href="#Watermarks-1" class="headerlink" title="Watermarks"></a>Watermarks</h3><h4 id="SleeperMark"><a href="#SleeperMark" class="headerlink" title="SleeperMark"></a>SleeperMark</h4><p><em>SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models</em></p>
<ul>
<li>TL;DR: 模型在适应新任务时很容易忘记先前学习的水印知识，SleeperMark提出一种抗微调的水印嵌入方法</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SleeperMark_Towards_Robust_Watermark_against_Fine-Tuning_Text-to-image_Diffusion_Models_CVPR_2025_paper.pdf" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://cvpr.thecvf.com/virtual/2025/poster/33491" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>大规模文本转图像 (T2I) 扩散模型的最新进展已赋能各种下游应用，包括风格定制、主题驱动的个性化和条件生成。由于 T2I 模型需要大量数据和计算资源进行训练，因此它们对其合法所有者而言构成了高价值的知识产权 (IP)，但也使其成为攻击者未经授权进行微调的目标，攻击者试图利用这些模型进行定制化、通常有利可图的应用。<br>现有的扩散模型 IP 保护方法通常包括嵌入水印模式，然后通过检查生成的输出或检查模型的特征空间来验证所有权。然而，在实际场景中，当嵌入水印的模型进行微调，且在验证过程中无法访问特征空间（即黑盒设置）时，这些技术本质上是无效的。<br>模型在适应新任务时很容易忘记先前学习的水印知识。为了应对这一挑战，我们提出了 <strong>SleeperMark</strong>，这是一个旨在 <strong>将弹性水印嵌入 T2I 扩散模型的新颖框架</strong> 。<br>SleeperMark 明确 <strong>引导模型将水印信息与其学习到的语义概念分离，从而使模型能够保留嵌入的水印，同时继续针对新的下游任务进行微调。</strong><br>我们进行了大量的实验，证明了 SleeperMark 在各种类型的扩散模型（包括潜在扩散模型（例如稳定扩散）和像素扩散模型（例如 DeepFloyd-IF））中的有效性，并展现出<strong>对下游微调和图像及模型层面各种攻击的稳健性，同时对模型的生成能力影响极小。</strong></p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014151057735.png"
                     
                ></p>
<h1 id="ICML-2025"><a href="#ICML-2025" class="headerlink" title="ICML 2025"></a>ICML 2025</h1><p>Submission Deadline: 2025.01.30</p>
<p>Official Site:</p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/papers.html" >ICML 2025 Papers<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml2025.vizhub.ai/" >ICML 2025 Posters Visualization<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h2 id="Attack-2"><a href="#Attack-2" class="headerlink" title="Attack"></a>Attack</h2><h3 id="Adversarial-Attack-2"><a href="#Adversarial-Attack-2" class="headerlink" title="Adversarial Attack"></a>Adversarial Attack</h3><h4 id="AdvI2I"><a href="#AdvI2I" class="headerlink" title="AdvI2I"></a>AdvI2I</h4><p><em>Adversarial Image Attack on Image-to-Image Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45719" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=It1AkQ6xEJ" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>idea: 针对I2I扩散模型，构造对抗性的输入图像来生成NSFW内容<blockquote>
<p>扩散模型的最新进展显著提升了图像合成的质量，但也带来了严重的安全隐患，尤其是不适宜工作场所观看（NSFW）内容的生成。<br>以往的研究表明，对抗性提示可用于生成NSFW内容。然而，此类对抗性文本提示通常很容易被基于文本的过滤器检测到，从而限制了其有效性。<br>本文揭示了一个此前被忽视的漏洞：针对图像到图像（I2I）扩散模型的对抗性图像攻击。我们提出了一种名为 <strong>AdvI2I</strong> 的新型框架，该框架通过操纵输入图像来诱导扩散模型生成NSFW内容。AdvI2I通过优化生成器来生成对抗性图像，从而绕过了现有的防御机制，例如安全潜在扩散（SLD），而无需修改文本提示。<br>此外，我们引入了 <strong>AdvI2I-Adaptive</strong>，这是一个增强版本，能够适应潜在的反制措施，并最大限度地降低对抗图像与 NSFW 概念嵌入之间的相似度，从而使攻击更具韧性，能够抵御防御措施。<br>通过大量实验，我们证明 AdvI2I 和 AdvI2I-Adaptive 都能有效绕过现有的安全措施，这凸显了加强安全措施以应对 I2I 扩散模型滥用问题的迫切性。</p>
</blockquote>
</li>
</ul>
<h4 id="DiffAdvMAP"><a href="#DiffAdvMAP" class="headerlink" title="DiffAdvMAP"></a>DiffAdvMAP</h4><p><em>DiffAdvMAP: Flexible Diffusion-Based Framework for Generating Natural Unrestricted Adversarial Examples</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/43968" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=q2s4DLsegO" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>与基于扰动的对抗样本（AE）相比，无限制对抗样本（UAE）对深度神经网络（DNN）构成了更大的威胁，因为它们可以在不受固定范数扰动预算限制的情况下对图像进行广泛的修改。<br>尽管目前基于扩散的方法能够生成比其他无限制攻击方法更自然的UAE，但由于这些方法针对特定攻击条件而设计，其整体有效性受到限制。此外，UAE的自然性仍有提升空间，因为这些方法主要侧重于利用扩散模型作为强先验来增强生成过程。<br>本文提出了一种名为基于扩散的对抗最大后验分布（DiffAdvMAP）的灵活框架，用于在各种场景下生成更自然的UAE。DiffAdvMAP通过从后验分​​布中采样图像来生成UAE，这是通过使用扩散模型学习到的真实数据的先验分布来近似UAE的后验分布来实现的。这一过程增强了UAE的自然性。通过引入对抗约束来确保攻击的有效性，DiffAdvMAP展现出卓越的攻击能力和防御鲁棒性。此外，我们还设计了重构约束来增强其灵活性，使其能够适应各种攻击场景。<br>在 <code>ImageNet</code> 数据集上的实验结果表明，与基线无约束对抗攻击方法相比，DiffAdvMAP在图像质量、灵活性和可迁移性之间取得了更好的平衡。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110154957149.png"
                     
                ></p>
<h3 id="Anti-Protective-Perturbation"><a href="#Anti-Protective-Perturbation" class="headerlink" title="Anti - Protective Perturbation"></a>Anti - Protective Perturbation</h3><h4 id="CAT-Contrastive-Adversarial-Training"><a href="#CAT-Contrastive-Adversarial-Training" class="headerlink" title="CAT (Contrastive Adversarial Training)"></a>CAT (Contrastive Adversarial Training)</h4><p><em>CAT: Contrastive Adversarial Training for Evaluating the Robustness of Protective Perturbations in Latent Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46408" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=5of0l7eUau" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/senp98/CAT" >code<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>idea: 针对Protective Perturbation的破解方法<blockquote>
<p>潜在扩散模型近年来在许多下游图像合成任务中展现出卓越的性能。然而，使用未经授权的数据定制潜在扩散模型会严重侵犯数据所有者的隐私权和知识产权。<br>对抗样本作为一种保护性扰动，通过在定制样本中引入难以察觉的噪声来阻止扩散模型有效地学习，从而防御未经授权的数据使用。<br>本文首先揭示了 <strong>对抗样本之所以能有效作为潜在扩散模型中的保护性扰动，其主要原因是它们会扭曲潜在表示</strong> ，并通过定性和定量实验验证了这一点。然后，我们提出了 <strong>一种利用lightweight adapters的对比对抗训练（CAT）方法，作为一种针对这些保护方法的adaptive attack，从而揭示了它们鲁棒性的不足。</strong><br>大量实验表明，我们的CAT方法显著降低了 Protective Perturbation 在定制中的有效性，促使业界重新思考并改进现有保护性扰动的鲁棒性。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110154134459.png"
                     
                ></p>
<h2 id="Defence-2"><a href="#Defence-2" class="headerlink" title="Defence"></a>Defence</h2><h3 id="Unlearn-1"><a href="#Unlearn-1" class="headerlink" title="Unlearn"></a>Unlearn</h3><h4 id="SAeUron"><a href="#SAeUron" class="headerlink" title="SAeUron"></a>SAeUron</h4><p><em>SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46380" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=6N0GxaKdX9" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/cywinski/SAeUron" >code<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型虽然功能强大，但可能会无意中生成有害或不良内容，引发严重的伦理和安全问题。<br>近年来，Machine Unlearn 方法虽然提供了潜在的解决方案，但往往缺乏透明度，难以理解其对基础模型的修改。<br>本文提出了一种名为 SAeUron 的新方法，该方法利用 <strong>稀疏自编码器 (SAE)</strong>  学习到的特征来去除文本到图像扩散模型中不必要的概念。<br>首先，我们证明，在扩散模型多个去噪时间步的激活值上进行无监督训练的 SAE 能够 <strong>捕获与特定概念对应的稀疏且可解释的特征</strong> 。<br>在此基础上，我们提出了 <strong>一种特征选择方法，该方法能够对模型激活值进行精确干预</strong> ，从而屏蔽目标内容，同时保持整体性能。<br>我们的评估结果表明，在 <code>UnlearnCanvas</code> 基准测试中，SAeUron 在概念和风格擦除方面优于现有方法，并且在 <code>I2P</code> 测试中能够有效去除裸露内容。此外，我们证明， <strong>使用单个SAE即可同时移除多个概念</strong> ，并且与其他方法相比，SAeUron能有效降低在对抗性攻击下生成不必要内容的可能性。code和checkpoint可在GitHub上获取。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110111911342.png"
                     
                ></p>
<h4 id="Adaptive-Median-Smoothing"><a href="#Adaptive-Median-Smoothing" class="headerlink" title="Adaptive Median Smoothing"></a>Adaptive Median Smoothing</h4><p><em>Adaptive Median Smoothing: Adversarial Defense for Unlearned Text-to-Image Diffusion Models at Inference Time</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45379" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=PdBEggnDIl" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本到图像（T2I）扩散模型引发了人们对生成不适宜内容（例如“裸露”）的担忧。尽管人们尝试通过遗忘技术消除不良概念，但这些遗忘后的模型仍然容易受到对抗性输入的影响，这些输入可能会重新生成此类内容。<br>为了保护遗忘后的模型，我们提出了一种新颖的推理时防御策略，以减轻对抗性输入的影响。具体而言，我们首先将确保遗忘扩散模型鲁棒性的挑战 <strong>重新表述为一个鲁棒回归问题</strong> 。基于使用 <strong>各向同性高斯噪声的朴素中值平滑回归鲁棒性方法</strong> ，我们开发了一个 <strong>包含各向异性噪声的广义中值平滑框架。</strong><br>基于此框架，我们引入了 <strong>一种基于词元的自适应中值平滑方法，该方法根据每个词元与目标概念的相关性动态调整噪声强度。</strong> 此外，为了提高推理效率，我们探索了在文本编码阶段实现这种自适应方法的可能性。<br>大量实验表明，我们的方法增强了对抗鲁棒性，同时保持了模型的效用和推理效率，优于基线防御技术。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110162553779.png"
                     
                ></p>
<h3 id="Concept-Mechanism"><a href="#Concept-Mechanism" class="headerlink" title="Concept Mechanism"></a>Concept Mechanism</h3><h4 id="Concept-Reachability"><a href="#Concept-Reachability" class="headerlink" title="Concept Reachability"></a>Concept Reachability</h4><p><em>Concept Reachability in Diffusion Models: Beyond Dataset Constraints</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/44102" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=nayOhK5DCg" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>尽管T2I模型的生成质量和复杂性取得了显著进步，但prompt并非总能带来预期的输出。<br>通过 <strong>直接控制中间模型激活</strong> 来控制模型行为已成为一种可行的替代方案，它能够 <strong>触及潜在空间中那些prompt可能无法触及的概念</strong> 。<br>本文提出了一系列实验，以加深我们对概念可达性的理解。我们设计了一个包含三个关键障碍的训练数据集： <strong>概念的稀缺性(scarcity of concepts)</strong> 、 <strong>图像描述中概念的欠定义(underspecification of concepts in the captions)</strong> 以及 <strong>包含关联概念的数据偏差(data biases with tied concepts)</strong> 。<br>我们的结果表明：<br>（i）潜在空间中概念的可达性呈现出明显的相变，只需少量样本即可实现可达性；<br>（ii）干预在潜在空间中的位置对可达性有显著影响，表明某些概念仅在转换的特定阶段才能被触及；<br>（iii）虽然提示能力会随着数据集质量的下降而迅速减弱，但通过控制激活，概念通常仍然能够可靠地被触及。<br>模型提供商可以利用这一点来绕过成本高昂的重新训练和数据集整理，转而创新面向用户的控制机制。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110113701743.png"
                     
                ></p>
<h4 id="ConceptAttention"><a href="#ConceptAttention" class="headerlink" title="ConceptAttention"></a>ConceptAttention</h4><p><em>ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45272" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=Rc7y9HFC34" >openreveiw<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>多模态扩散变换器（DiT）的丰富表征是否展现出增强其可解释性的独特属性？<br>我们提出了一种名为 <strong>ConceptAttention</strong> 的新方法，该方法 <strong>利用 DiT 注意力层的表达能力生成高质量的显著性图，从而精确定位图像中的文本概念。</strong><br>ConceptAttention 无需额外训练，即可重新利用 DiT 注意力层的参数生成高度上下文相关的概念嵌入，并取得了一项重要发现：与常用的交叉注意力图相比， <strong>在 DiT 注意力层的输出空间进行线性投影可以显著提高显著性图的清晰度</strong> 。<br>ConceptAttention 甚至在零样本图像分割基准测试中取得了最先进的性能，在 <code>ImageNet-Segmentation</code> 数据集上优于其他 15 种零样本可解释性方法。ConceptAttention 适用于流行的图像模型，甚至可以无缝泛化到视频生成。我们的工作首次证明， <strong>多模态 DiT 的表征具有高度的视觉迁移性，可以应用于分割等视觉任务。</strong></p>
</blockquote>
</li>
</ul>
<h4 id="Mechanisms-of-Projective-Composition-of-Diffusion-Models"><a href="#Mechanisms-of-Projective-Composition-of-Diffusion-Models" class="headerlink" title="Mechanisms of Projective Composition of Diffusion Models"></a>Mechanisms of Projective Composition of Diffusion Models</h4><p><em>Mechanisms of Projective Composition of Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45339" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=QV0PcBbfTd" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>本文研究扩散模型中 <strong>组合(composition)</strong> 的理论基础，尤其关注 <strong>分布外外推(out-of-distribution extrapolation)</strong> 和 <strong>长度泛化(length-generalization)</strong> 。<br>先前的研究表明，通过 linear score combination 来 <strong>composing distributions</strong> 可以取得令人满意的结果，包括在某些情况下实现长度泛化(length-generalization)（Du et al., 2023; Liu et al., 2022）。<br>然而，我们对这种组合如何以及为何有效，其理论理解仍然不完整。事实上，组合“有效”的含义甚至尚不完全清楚。本文旨在弥补这些根本性的空白。<br>首先，我们精确定义了组合的一种可能的理想结果，称之为 <strong>投射性组合(projective composition)</strong> 。然后，我们研究：<br>（1）linear score combination何时能够被证明实现 projective composition；<br>（2）反向扩散采样是否能够生成理想的组合；<br>（3）组合失效的条件。<br>我们将理论分析与先前的经验观察联系起来，这些经验观察表明， <strong>组合在某些情况下有效，而在另一些情况下无效</strong> ，但当时的原因尚不明确。最后，我们提出了 <strong>一种简单的启发式方法 来 帮助预测新组合的成功或失败。</strong></p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110152526373.png"
                     
                ></p>
<h4 id="Can-Diffusion-Models-Learn-Hidden-Inter-Feature-Rules-Behind-Images"><a href="#Can-Diffusion-Models-Learn-Hidden-Inter-Feature-Rules-Behind-Images" class="headerlink" title="Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?"></a>Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?</h4><p><em>Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45949" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=ERU7QgD6gc" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>尽管扩散模型（DM）在数据生成方面取得了显著成功，但它们在某些特定情况下仍会出现输出不尽如人意的缺陷。我们重点关注其中一个局限性：<strong>DMs学习图像特征之间隐藏规则的能力。</strong><br>具体而言，对于具有相关特征的图像数据 x 和y（例如，太阳的高度x 以及阴影的长度y），我们研究决策者 <strong>是否能够准确地捕捉特征间的规则 P(y|x)</strong> 。对主流图像扩散器（例如 Stable Diffusion 3.5）的实证评估揭示了其普遍存在的缺陷，例如光照-阴影关系不一致以及物体与镜面反射不匹配。<br>受此启发，我们设计了 <strong>4个具有强相关特征的合成任务</strong> 来评估 <strong>图像扩散器的规则学习能力</strong> 。<br>大量实验表明，虽然图像扩散器能够识别粗粒度规则，但它们在 <strong>处理细粒度规则时却表现不佳</strong> 。我们的理论分析表明， <strong>通过去噪分数匹配（DSM）训练的图像扩散器在学习隐藏规则时会持续出现误差，因为 DSM 的目标函数与规则一致性并不兼容。</strong><br>为了缓解这一问题，我们引入了一种常用技术——在采样过程中 <strong>加入额外的分类器指导，该技术能够取得（有限的）改进。</strong> 我们的分析表明，分类器难以捕捉细粒度规则的细微信号，这为未来的研究提供了新的思路。</p>
</blockquote>
</li>
</ul>
<h3 id="Privacy"><a href="#Privacy" class="headerlink" title="Privacy"></a>Privacy</h3><h4 id="Ambient-Diffusion"><a href="#Ambient-Diffusion" class="headerlink" title="Ambient Diffusion"></a>Ambient Diffusion</h4><p><em>Does Generation Require Memorization? Creative Diffusion Models using Ambient Diffusion</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45841" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=GGPM0z3dhU" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>Ambient [ˈæmbiənt] adj. 环境的,环境光<blockquote>
<p>大量经验证据表明，目前最先进的扩散建模范式会导致模型记忆训练集，尤其是在训练集较小的情况下。<br>以往缓解记忆问题的方法通常会导致图像质量下降。是否有可能获得强大且富有创造力的生成模型，i.e. 既能实现高生成质量又能降低记忆效应的模型？<br>尽管目前的研究结果并不乐观，但我们在平衡 <strong>保真度(fidelity)</strong> 和 <strong>记忆效应(memorization)</strong> 方面取得了显著进展。<br>我们首先从理论上证明， <strong>扩散模型中的记忆效应仅在低噪声尺度（通常用于生成高频细节）的去噪问题中是必要的。</strong><br>基于这一理论见解，我们提出了一种简单而有原则的方法， <strong>利用高噪声尺度的噪声数据来训练扩散模型。</strong> 我们证明，对于文本条件模型和非条件模型，以及各种数据可用性设置，我们的方法都能 <strong>在不降低图像质量的前提下显著降低记忆效应。</strong></p>
</blockquote>
</li>
</ul>
<h3 id="Steganography"><a href="#Steganography" class="headerlink" title="Steganography"></a>Steganography</h3><h4 id="MDDM"><a href="#MDDM" class="headerlink" title="MDDM"></a>MDDM</h4><p><em>MDDM: Practical Message-Driven Generative Image Steganography Based on Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/45483" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=NniXePXVXw" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>生成式图像隐写术（GIS）是一种新兴技术，它将秘密信息隐藏在图像生成过程中。与基于生成对抗网络（GAN）或基于流的GIS方案相比，基于扩散模型的解决方案能够生成高质量且更多样化的图像，因此近年来备受关注。<br>然而，以往的GIS方案在提取精度、可控性和实用性方面仍面临挑战。为了解决上述问题，本文提出了一种基于扩散模型的 <strong>实用型消息驱动GIS框架(Message-Driven GIS framework based on Diffusion Models)，称为MDDM。</strong><br>具体而言，我们利用卡丹格栅将信息编码为高斯噪声，作为图像生成的初始输入，使用户能够在无需额外训练的情况下，通过可控的提示生成多样化的图像。在信息提取过程中，接收者只需使用预先共享的卡丹格栅进行精确的扩散反演，即可恢复信息，而无需图像生成种子或提示。<br>实验结果表明，MDDM在精度、可控性、实用性和安全性方面均具有显著优势。凭借灵活的策略，MDDM几乎总能达到100%的准确率。此外，MDDM展现出一定的鲁棒性，并具有在水印任务中应用的潜力。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110155718322.png"
                     
                ></p>
<h2 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h2><h3 id="Adversarial-Purification"><a href="#Adversarial-Purification" class="headerlink" title="Adversarial Purification"></a>Adversarial Purification</h3><h4 id="Diffusion-based-Adversarial-Purification-from-the-Perspective-of-the-Frequency-Domain"><a href="#Diffusion-based-Adversarial-Purification-from-the-Perspective-of-the-Frequency-Domain" class="headerlink" title="Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain"></a>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</h4><p><em>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46096" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=Bm706VlAtU" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/media/icml-2025/Slides/46096.pdf" >slides<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>基于扩散的对抗性净化方法试图通过前向过程将对抗性扰动淹没到部分各向同性噪声中，然后通过反向过程恢复干净图像。 <strong>由于像素域中缺乏对抗性扰动的分布信息，通常不可避免地会破坏图像的正常语义。</strong><br>我们转向频域视角，将图像分解为 <strong>幅度谱</strong> 和 <strong>相位谱</strong> 。 <strong>我们发现，对于这两个谱，对抗性扰动造成的损害都随频率单调增加</strong> 。这意味着我们可以 <strong>从受损较小的频率分量中提取原始干净样本的内容和结构信息。</strong><br>同时，理论分析表明，现有的净化方法不加区分地破坏所有频率分量，导致图像过度受损。因此， <strong>我们提出了一种净化方法，该方法能够在消除对抗性扰动的同时，最大程度地保留原始图像的内容和结构。</strong><br>具体而言，在逆向过程的每个时间步，对于幅度谱， <strong>我们将估计图像幅度谱的低频分量替换为对抗图像的相应部分。</strong> 对于相位谱， <strong>我们将估计图像的相位投影到对抗图像相位谱的指定范围内，重点关注低频部分。</strong><br>大量实验的经验证据表明，我们的方法显著优于目前大多数防御方法。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110160631611.png"
                     
                ></p>
<h3 id="Unlearn-2"><a href="#Unlearn-2" class="headerlink" title="Unlearn"></a>Unlearn</h3><h4 id="Not-All-Wrong-is-Bad"><a href="#Not-All-Wrong-is-Bad" class="headerlink" title="Not All Wrong is Bad"></a>Not All Wrong is Bad</h4><p><em>Not All Wrong is Bad: Using Adversarial Examples for Unlearning</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46097" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=BkrIQPREkn" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>由于众多隐私法规的出现，机器遗忘（用户可以请求删除遗忘数据集）变得越来越重要。早期关于“精确”遗忘（例如，重新训练）的研究会带来巨大的计算开销。然而，尽管计算成本低廉，“近似”方法在效果上仍不及精确遗忘：生成的模型在遗忘数据集和测试（即未见过的）数据集上都无法获得可比拟的准确率和预测置信度。<br>基于此，我们提出了一种新的遗忘方法—— <strong>对抗机器遗忘（Adversarial Machine UNlearning, AMUN）</strong> ，其性能优于目前最先进的图像分类方法。AMUN 通过在相应的对抗样本上微调模型来降低模型对遗忘样本的置信度。对抗样本自然属于模型对输入空间施加的分布；通过对与对应遗忘样本最接近的对抗样本进行模型微调， <strong>(a) 将模型决策边界的改变局限在每个遗忘样本附近</strong> ， <strong>(b) 避免对模型的全局行为造成剧烈改变，从而保持模型在测试样本上的准确率。</strong><br>使用 AMUN 对随机选取的 10% CIFAR-10 样本进行遗忘，我们观察到，即使是最先进的成员推理攻击也无法超越随机猜测。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110164018287.png"
                     
                ></p>
<h4 id="Knowledge-Swapping-via-Learning-and-Unlearning"><a href="#Knowledge-Swapping-via-Learning-and-Unlearning" class="headerlink" title="Knowledge Swapping via Learning and Unlearning"></a>Knowledge Swapping via Learning and Unlearning</h4><p><em>Knowledge Swapping via Learning and Unlearning</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://icml.cc/virtual/2025/poster/46135" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://openreview.net/forum?id=B3zlIHdnER" >openreview<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/xingmingyu123456/KnowledgeSwapping" >code<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>我们提出了知识交换（Knowledge Swapping）这一新颖的任务，旨在通过允许模型遗忘用户指定的信息、保留关键知识并同时获取新知识，从而有选择地调节预训练模型的知识。通过深入分析特征层级结构，我们发现 <strong>增量学习通常从低级表征逐步过渡到高级语义</strong> ，而 <strong>遗忘则倾向于以相反的方向发生——从高级语义开始，逐步向下过渡到低级特征。</strong><br>基于此，我们提出采 用 <strong>“先学习后遗忘”（Learning Before Forgetting）策略</strong> 来评估知识交换任务的性能。在图像分类、目标检测和语义分割等多种任务上的综合实验验证了所提出策略的有效性。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251110163750086.png"
                     
                ></p>
<h1 id="ICCV-2025"><a href="#ICCV-2025" class="headerlink" title="ICCV 2025"></a>ICCV 2025</h1><p>Submission Deadline: 2025.03.07</p>
<p>Official Site:</p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/papers.html" >ICCV 2025 Papers<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h2 id="Attack-3"><a href="#Attack-3" class="headerlink" title="Attack"></a>Attack</h2><h3 id="Adversarial-Attack-3"><a href="#Adversarial-Attack-3" class="headerlink" title="Adversarial Attack"></a>Adversarial Attack</h3><h4 id="DIA-DDIM-Inversion-Attack"><a href="#DIA-DDIM-Inversion-Attack" class="headerlink" title="DIA (DDIM Inversion Attack)"></a>DIA (DDIM Inversion Attack)</h4><p><em>DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2510.00778" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/1951" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/sohn1029/DIA" >code<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型已被证明是强大的表征学习器，在多个领域展现出最佳性能。除了加速采样之外，DDIM 还能将真实图像反转回其潜在代码。此反转操作的一个直接继承应用是真实图像编辑，其中反转产生的潜在轨迹可用于编辑图像的合成。不幸的是，这种实用工具使恶意用户能够更轻松地自由合成虚假信息或深度伪造内容，从而助长了不道德、滥用以及侵犯隐私和版权内容的传播。<br>虽然 AdvDM 和 Photoguard 等防御性算法已被证明可以破坏这些图像的扩散过程，但它们的目标与测试时的迭代去噪轨迹之间的不一致导致了较弱的破坏性能。<br>在本文中，我们提出了一种 **攻击 integrated DDIM trajectory path 的 DDIM Inversion Attack (DIA)**。<br>我们的研究结果支持了这种有效的破坏方法，其性能超越了以往各种编辑方法的防御方法。我们相信，我们的框架和结果可以为业界和研究界提供切实可行的防御方法，以抵御人工智能的恶意使用。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014182737040.png"
                     
                ></p>
<h4 id="ZIUM-Zero-shot-Intent-aware-adversarial-attack-on-Unlearned-Models"><a href="#ZIUM-Zero-shot-Intent-aware-adversarial-attack-on-Unlearned-Models" class="headerlink" title="ZIUM (Zero-shot Intent-aware adversarial attack on Unlearned Models)"></a>ZIUM (Zero-shot Intent-aware adversarial attack on Unlearned Models)</h4><p><em>ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.21985" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/1002" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://youtu.be/IUtjhI5zsZ0" >video<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>机器学习 (MU) 会从深度学习模型中移除特定数据点或概念，以增强隐私保护并防止生成敏感内容。对抗性提示可以利用 Unlearn Models(UMs) 生成包含已移除概念的内容，从而构成重大安全风险。<br>然而，现有的对抗性攻击方法仍然难以生成符合攻击者意图的内容，同时识别 successful prompts 的计算成本也很高。<br>为了应对这些挑战，我们提出了 <strong>ZIUM (a Zero-shot Intent-aware adversarial attack on Unlearned Models)</strong> ，它能够 <strong>灵活地定制目标攻击图像</strong> 以反映攻击者的意图。此外，ZIUM 支持 <strong>Zero-shot对抗攻击</strong>，而无需针对先前攻击过的遗忘概念进行进一步优化。<br>在各种 MU 场景下的评估表明，ZIUM 能够 <strong>有效地根据用户意图提示成功定制内容，同时获得比现有方法更高的攻击成功率</strong> 。此外，其零样本对抗攻击 <strong>显著缩短了针对先前攻击过的未学习概念的攻击时间。</strong></p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014183655722.png"
                     
                ></p>
<h3 id="Backdoor-Attack"><a href="#Backdoor-Attack" class="headerlink" title="Backdoor Attack"></a>Backdoor Attack</h3><h4 id="BadVideo"><a href="#BadVideo" class="headerlink" title="BadVideo"></a>BadVideo</h4><p><em>BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation</em></p>
<ul>
<li><a href="">pdf</a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/833" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本转视频 (T2V) 生成模型发展迅速，并在娱乐、教育和市场营销等领域得到了广泛应用。然而，这些模型的对抗性漏洞却鲜少被深入研究。<br>我们观察到，<strong>在 T2V 生成任务中，生成的视频通常包含大量文本提示中未明确指定的冗余信息，例如环境元素、次要对象和其他细节，这为恶意攻击者嵌入隐藏的有害内容提供了机会。</strong> 利用这种固有的冗余性，我们推出了 <strong>BadVideo</strong>，这是第一个专为 T2V 生成量身定制的后门攻击框架。<br>我们的攻击专注于通过两种关键策略设计目标对抗性输出：**(1) 时空组合，结合不同的时空特征** 来编码恶意信息； <strong>(2) 动态元素变换，通过引入冗余元素随时间的变化</strong> 来传达恶意信息。基于这些策略，攻击者的恶意目标可以 <strong>与用户的文本指令无缝集成，从而提供高度的隐蔽性。</strong><br>此外，<strong>通过利用视频的时间维度，我们的攻击成功规避了主要分析单帧空间信息的传统内容审核系统。</strong><br>大量实验表明， <strong>BadVideo 在保留原始语义并在干净输入上保持优异性能的同时，实现了较高的攻击成功率。</strong> 总而言之，我们的工作揭示了 T2V 模型的对抗性弱点，并提醒人们注意潜在的风险和误用。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014184206208.png"
                     
                ></p>
<h2 id="Defence-3"><a href="#Defence-3" class="headerlink" title="Defence"></a>Defence</h2><h3 id="Anomaly-Detection-for-Adversarial-and-Backdoor-Attacks"><a href="#Anomaly-Detection-for-Adversarial-and-Backdoor-Attacks" class="headerlink" title="Anomaly Detection for Adversarial and Backdoor Attacks"></a>Anomaly Detection for Adversarial and Backdoor Attacks</h3><h4 id="DADet-Diffusion-Anomaly-Detection"><a href="#DADet-Diffusion-Anomaly-Detection" class="headerlink" title="DADet (Diffusion Anomaly Detection)"></a>DADet (Diffusion Anomaly Detection)</h4><p><em>DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection</em></p>
<ul>
<li>暂无公开pdf</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/1812" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>图像条件扩散模型虽然展现出卓越的生成能力，但在面对后门攻击和对抗攻击时却表现出极高的脆弱性。<br>本文定义了一种名为 <strong>扩散异常(diffusion anomaly)</strong> 的场景，即 <strong>在受到攻击的情况下，反向去噪过程的生成结果与正常结果存在显著偏差。</strong><br>通过分析扩散异常的形成机制，我们揭示了 <strong>扰动如何在反向去噪过程(reverse process)中被放大并在结果中累积</strong>。基于分析，我们揭示了 <strong>发散性和同质性(divergence and homogeneity)现象，这导致扩散过程(diffusion process)显著偏离正常过程，多样性下降</strong> 。利用这两种现象，我们提出了一种名为 <strong>扩散异常检测(DADet)</strong> 的方法，可以 <strong>有效地检测后门攻击和对抗攻击</strong> 。<br>大量实验表明，我们的方案对后门攻击和对抗攻击均具有优异的防御性能。具体而言，对于后门攻击检测，我们的方法在包括MS COCO和CIFAR-10在内的不同数据集上获得了99%的F1分数。对于对抗样本的检测，在 MS COCO 和 Places365 数据集上分别评估的三次对抗攻击和两项不同任务中，F1 分数超过 84%。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014185534336.png"
                     
                ></p>
<h3 id="Unlearn-3"><a href="#Unlearn-3" class="headerlink" title="Unlearn"></a>Unlearn</h3><h4 id="TRCE-Towards-Reliable-Malicious-Concept-Erasure"><a href="#TRCE-Towards-Reliable-Malicious-Concept-Erasure" class="headerlink" title="TRCE (Towards Reliable Malicious Concept Erasure)"></a>TRCE (Towards Reliable Malicious Concept Erasure)</h4><p><em>TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.07389" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/21" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://github.com/ddgoodgood/TRCE" >code<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本到图像扩散模型的最新进展使得生成逼真的图像成为可能，但也存在生成恶意内容（例如 NSFW 图片）的风险。为了降低风险，人们研究了概念擦除方法，以帮助模型忘记特定概念。<br>然而，目前的研究难以完全擦除隐含在提示（例如隐喻表达或对抗性提示）中的恶意概念，同时保留模型的正常生成能力。<br>为了应对这一挑战，我们的研究提出了 <strong>TRCE</strong>，它 <strong>使用两阶段概念擦除策略来在可靠擦除和知识保存之间实现有效的权衡</strong> 。<br>首先，TRCE 从擦除隐含在文本提示中的恶意语义开始。<strong>通过确定有效的映射目标（即 [EoT] 嵌入），我们优化了交叉注意力层，将恶意提示映射到上下文相似但概念安全的提示。</strong> 此步骤可防止模型在去噪过程中受到恶意语义的过度影响。在此基础上，考虑到扩散模型采样轨迹的确定性，<strong>TRCE 通过对比学习进一步引导早期去噪预测向安全方向发展，远离不安全方向，从而进一步避免恶意内容的生成。</strong><br>最后，我们在多个恶意概念擦除基准上对 TRCE 进行了全面的评估，结果证明了其 <strong>在擦除恶意概念方面的有效性，同时更好地保留了模型原有的生成能力</strong> 。本文涵盖了模型生成的内容中可能包含攻击性内容。</p>
</blockquote>
</li>
</ul>
<!-- ![](https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/21.png) -->
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014154352078.png"
                     
                ></p>
<h4 id="SuMa-Subspace-Mapping"><a href="#SuMa-Subspace-Mapping" class="headerlink" title="SuMa (Subspace Mapping)"></a>SuMa (Subspace Mapping)</h4><p><em>SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.05625" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/418" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://youtu.be/SeqAEhOZbEQ" >video<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>文本到图像扩散模型的快速发展引发了人们对其可能被滥用于生成有害或未经授权内容的担忧。为了解决这些问题，人们提出了几种概念擦除方法。<br>然而，大多数方法都无法实现 <strong>完整性（即完全删除目标概念的能力）</strong> 和 <strong>有效性（即保持图像质量）</strong> 。虽然近期有少数技术成功实现了针对 NSFW 概念的上述目标，但没有一种技术能够处理诸如受版权保护的人物或名人等狭义概念。<br>消除这些狭义概念对于解决版权和法律问题至关重要。然而，由于这些概念与非目标相邻概念的距离很近，因此从扩散模型中删除它们具有挑战性，需要更精细的操作。在本文中，我们介绍了<strong>子空间映射（SuMa）</strong>，这是一种新颖的方法，专门用于实现 <strong>擦除这些狭义概念的完整性和有效性。</strong><br><strong>SuMa 首先得出一个代表要消除的概念的目标子空间，然后通过将其映射到一个参考子空间，使两者之间的距离最小化，从而对其进行中和</strong>。这种映射可确保目标概念被完全消除，同时保持图像质量。<br>我们用 SuMa 在四项任务中进行了广泛的实验：<strong>子类消除</strong>、<strong>名人消除</strong>、<strong>艺术风格消除</strong> 和 <strong>实例消除</strong>，并将实验结果与当前最先进的方法进行了比较。我们的方法不仅在图像质量方面优于那些注重有效性的方法，而且还取得了与注重完整性的方法相当的结果。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014155700535.png"
                     
                ></p>
<h4 id="Meta-Unlearning"><a href="#Meta-Unlearning" class="headerlink" title="Meta-Unlearning"></a>Meta-Unlearning</h4><p><em>Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.12777" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/2472" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>随着扩散模型 (DMs) 的快速发展，人们正在做出巨大努力来从预训练的 DMs 中清除有害或受版权保护的概念，以防止潜在的模型滥用。然而，据观察，即使 DMs 在发布前已正确清除，恶意的微调也会破坏这一过程，导致 DM重新学习 Unlearn的概念。<br>发生这种情况的部分原因是 DM 中保留的某些良性概念（例如“皮肤”）与 Unlearn的概念（例如“裸体”）相关，从而有助于通过微调进行重新学习。<br>为了解决这个问题，我们提出 <strong>meta-unlearning</strong> on DMs。直观地说，meta-unlearned DMs <strong>在按原样使用时应该表现得像 unlearned DMs；</strong> 此外，如果 meta-unlearned DMs **对未学习的概念进行恶意微调时，其中保留的相关良性概念则将触发自毁(self-destruct)**，从而阻碍对 unlearned concepts 的重新学习。<br>我们的 <strong>meta-unlearning 框架与大多数现有的 Unlearn 方法兼容，只需添加一个易于实现的元目标(meta objective)即可。</strong><br>我们通过对稳定扩散模型（SD-v1-4 和 SDXL）中的元反学习概念进行实证实验来验证我们的方法，并得到了大量消融研究的支持。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014193128529.png"
                     
                ></p>
<h4 id="EraseBench"><a href="#EraseBench" class="headerlink" title="EraseBench"></a>EraseBench</h4><p><em>Erasing More Than Intended? How Concept Erasure Degrades the Generation of Non-Target Concepts</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2501.09833v2" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/1222" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>概念擦除技术因其从文本转图像模型中移除不想要概念的潜力而备受关注。虽然这些方法在受控环境中通常表现出良好的效果，但它们在实际应用中的稳健性和部署适用性仍不确定。<br>在本研究中，我们 <strong>(1) 发现了 评估 净化模型 的关键缺陷</strong> ，尤其是在评估其在不同概念维度上的表现方面； <strong>(2) 系统地分析了 擦除后文本转图像模型的失效模式</strong> 。我们重点研究了概念移除对不同层次互联关系（包括视觉相似、二项式和语义相关概念）中的非目标概念造成的意外后果。<br>为了更全面地评估概念擦除，我们引入了 <strong>EraseBench</strong>，这是 <strong>一个多维框架，旨在严格评估擦除后的文本转图像模型</strong> 。它包含 <strong>100</strong> 多个不同的概念、精心策划的种子提示以确保可重复的图像生成，以及用于基于模型评估的专用评估提示。我们的框架 <strong>与一套强大的评估指标相结合</strong> ，对 <strong>概念擦除的有效性及其对模型行为的长期影响</strong> 进行了全面而深入的分析。<br>我们的研究结果揭示了 <strong>概念纠缠现象，其中擦除导致非目标概念的意外抑制，导致溢出退化，表现为扭曲和生成质量下降</strong>。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014154506706.png"
                     
                ></p>
<h4 id="Holistic-Unlearning-Benchmark"><a href="#Holistic-Unlearning-Benchmark" class="headerlink" title="Holistic Unlearning Benchmark"></a>Holistic Unlearning Benchmark</h4><p><em>Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2410.05664" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/546" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://saemi410.github.io/HUB/" >webpage<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>随着文本到图像的传播模型获得广泛的商业应用，人们越来越担心不道德或有害的使用，包括未经授权生成受版权保护的内容或敏感内容。Concept unlearning 已成为应对这些挑战的一种有前途的解决方案，它从预训练模型中去除不需要的和有害的信息。<br>然而， <strong>之前的评估主要关注是否在保留图像质量的同时删除了目标概念，而忽略了更广泛的影响，例如意想不到的副作用。</strong><br>在本文中，我们提出了 <strong>Holistic Unlearning Benchmark (HUB)</strong> ，这是一个全面的框架，用于评估Unlearn方法的 <strong>6个关键维度：faithfulness, alignment, pinpoint-ness, multilingual robustness, attack robustness</strong><br>我们的基准涵盖 <strong>33 个目标概念，每个概念包含 16,000 个提示</strong> ，涵盖 <strong>4个类别：Celebrity, Style, Intellectual Property, and NSFW</strong><br>我们的调查显示，没有一种方法在所有评估标准上都表现出色。通过发布我们的评估代码和数据集，我们希望激发该领域的进一步研究，从而产生更可靠、更有效的Unlearn方法。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014193755432.png"
                     
                ></p>
<h3 id="Watermark-1"><a href="#Watermark-1" class="headerlink" title="Watermark"></a>Watermark</h3><h4 id="PlugMark"><a href="#PlugMark" class="headerlink" title="PlugMark"></a>PlugMark</h4><p><em>PlugMark: A Plug-in Zero-Watermarking Framework for Diffusion Models</em></p>
<ul>
<li>暂无公开pdf</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/356" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://youtu.be/jDn0tr5EjWM" >video<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型极大地推动了图像合成领域的发展，其知识产权 (IP) 的保护也成为至关重要的问题。<br>现有的 IP 保护方法主要集中在 通过改变扩散过程的结构来将水印嵌入到生成的图像中。然而，这些方法不可避免地会 <strong>损害生成图像的质量，并且特别容易受到微调攻击，尤其是对于稳定扩散 (SD) 等开源模型。</strong><br>本文提出了 <strong>PlugMark，一个用于扩散模型的新型插件式零水印框架</strong> 。PlugMark 的核心思想基于 <strong>两个观察结果：分类器可以通过其决策边界唯一地表征，扩散模型可以通过从训练数据中获得的知识唯一地表示。</strong> 在此基础上，我们引入了一个<strong>扩散知识提取器，它可以插入到扩散模型中以提取其知识并输出分类结果。</strong> 随后，PlugMark 基于该分类结果生成边界表示，作为零失真水印，唯一地表示决策边界，进而表示扩散模型的知识。<strong>由于只有提取器需要训练，因此原始扩散模型的性能不受影响。</strong><br>大量实验结果表明，PlugMark 可以从原始模型及其后处理版本中稳健地提取高置信度零水印，同时有效地将它们与非后处理的扩散模型区分开来。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014193836627.png"
                     
                ></p>
<h3 id="Copyright"><a href="#Copyright" class="headerlink" title="Copyright"></a>Copyright</h3><h4 id="CopyrightShield"><a href="#CopyrightShield" class="headerlink" title="CopyrightShield"></a>CopyrightShield</h4><p><em>CopyrightShield: Enhancing Diffusion Model Security Against Copyright Infringement Attacks</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.01528" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/1902" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>扩散模型因其在图像合成等领域卓越的数据生成能力而备受关注。然而，近期研究表明，扩散模型易受版权侵权攻击。 <strong>攻击者会将经过策略性修改的非侵权图像注入训练集，诱导模型在特定“毒害”字幕的提示下生成侵权内容。</strong><br>针对此问题，我们首先提出了一个防御框架—— <strong>CopyrightShield</strong> ，以防御上述攻击。具体而言，我们 <strong>分析了扩散模型的记忆机制，发现攻击利用模型对特定空间位置和提示的过拟合，导致其在后门触发下生成毒害样本。</strong><br>基于此，我们提出了 <strong>一种基于空间掩蔽和数据归因的毒害样本检测方法，以量化毒害风险并准确识别隐藏的后门样本。</strong> 为了进一步降低对毒害特征的记忆，我们引入了 <strong>一种自适应优化策略，将动态惩罚项集成到训练损失中，在保持生成性能的同时降低对侵权特征的依赖。</strong><br>实验结果表明，CopyrightShield在两种攻击场景下显著提升了中毒样本的检测性能，平均F1-scores达到0.665，首次攻击时间（FAE）延迟115.2%，版权侵权率（CIR）降低56.7%。相比于扩散模型中的SoTA后门防御，防御效果提升约25%，展现了其在提升扩散模型安全性方面的优越性和实用性。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014195000515.png"
                     
                ></p>
<h2 id="Other-1"><a href="#Other-1" class="headerlink" title="Other"></a>Other</h2><h3 id="Unlearn-4"><a href="#Unlearn-4" class="headerlink" title="Unlearn"></a>Unlearn</h3><p>趋势：解决 Machine Unlearn 的两个目标的优化冲突问题</p>
<ul>
<li>遗忘特定概念&#x2F;数据</li>
<li>保持总体性能（减小对其他概念的干扰）</li>
</ul>
<h4 id="MUNBa-Machine-Unlearning-via-Nash-Bargaining"><a href="#MUNBa-Machine-Unlearning-via-Nash-Bargaining" class="headerlink" title="MUNBa (Machine Unlearning via Nash Bargaining)"></a>MUNBa (Machine Unlearning via Nash Bargaining)</h4><p><em>MUNBa: Machine Unlearning via Nash Bargaining</em></p>
<ul>
<li>TL;DR: 用纳什谈判解的方法，优化MU的两个目标</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.15537" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/224" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>机器遗忘（MU）旨在有选择性地清除模型中的有害行为，同时保留模型的整体效用。作为一个多任务学习问题，MU 需要在 <strong>遗忘特定概念&#x2F;数据</strong> 和 <strong>保持总体性能</strong> 这两个目标之间取得平衡。<br>为了解决梯度冲突和优势问题，我们 <strong>将 MU 重新表述为双人合作博弈</strong> ，即遗忘博弈者和保留博弈者通过梯度建议来最大化他们的整体收益并平衡他们的贡献。<br>我们对 MU 的表述 <strong>保证了均衡解，任何偏离最终状态的情况都会导致双方总体目标的降低，从而确保每个目标的最优性。</strong><br>我们用 ResNet、视觉语言模型 CLIP 和文本到图像扩散模型进行了大量实验，结果表明我们的方法优于最先进的 MU 算法，在遗忘和保持之间实现了更好的权衡。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014191014208.png"
                     
                ></p>
<h4 id="Water4MU"><a href="#Water4MU" class="headerlink" title="Water4MU"></a>Water4MU</h4><p><em>Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.10065" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/517" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>随着人们对被遗忘权的需求日益增长，Machine Unlearn (MU) 已成为增强信任和法规遵从性的重要工具，因为它能够从机器学习 (ML) 模型中消除敏感数据的影响。然而，大多数 MU 算法主要依靠 <strong>in-training methods</strong> 来调整模型权重，而对 <strong>data-level adjustments</strong> 的探索有限。<br>为了弥补这一差距，我们提出了一种新颖的方法，<strong>利用 digital watermarking 策略性地修改数据内容，来促进 MU。</strong> 通过集成 watermarking，我们建立了 <strong>一种受控的Unlearn机制，该机制能够精确删除指定数据，同时保持不相关任务的模型效用。</strong><br>我们首先研究了带水印的数据对 MU 的影响，发现 MU 可以有效地推广到带水印的数据。在此基础上，我们引入了 <strong>一个有利于反学习的水印框架，称为 Water4MU，以提高反学习的有效性。</strong> Water4MU 的核心是 <strong>一个双层优化 (bi-level optimization, BLO) 框架：在上层，水印网络经过优化以最小化遗忘难度；而在下层，模型本身则独立于水印进行训练。</strong><br>实验结果表明，Water4MU 在图像分类和图像生成任务中均能有效进行 MU。值得注意的是，它在具有挑战性的 MU 场景（即所谓的“挑战性遗忘”）中的表现优于现有方法。</p>
</blockquote>
</li>
</ul>
<h4 id="LUR-Learning-to-Unlearn-while-Retaining"><a href="#LUR-Learning-to-Unlearn-while-Retaining" class="headerlink" title="LUR (Learning to Unlearn while Retaining)"></a>LUR (Learning to Unlearn while Retaining)</h4><p><em>Learning to Unlearn while Retaining: Combating Gradient Conflicts in Machine Unlearning</em></p>
<ul>
<li><a class="link"   target="_blank" rel="noopener" href="https://arxiv.org/pdf/2503.06339" >pdf<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/2659" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link"   target="_blank" rel="noopener" href="https://youtu.be/dRaQt1YCnTo" >video<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>机器学习的 Unlearning 最近备受关注，其目标是选择性地移除与特定数据相关的知识，同时保留模型在剩余数据上的性能。<br>这一过程中的一个根本挑战是 <strong>如何平衡有效的遗忘和知识保留</strong> ，因为对这些相互竞争的目标进行简单的优化可能会导致梯度冲突，从而阻碍收敛并降低整体性能。<br>为了解决这个问题，我们提出了 <strong>Learning to Unlearn while Retaining</strong> ，旨在 <strong>缓解遗忘和保留目标之间的梯度冲突</strong>。<br>我们的方法通过一种在所提框架内自然形成的 <strong>隐式梯度正则化机制，策略性地避免了冲突。</strong> 这可以防止遗忘和保留之间出现梯度冲突，从而实现有效的遗忘，同时保留模型的效用。<br>我们在判别任务和生成任务中验证了我们的方法，证明了它在不影响剩余数据性能的情况下实现Unlearn的有效性。我们的结果突出了避免此类梯度冲突的优势，优于未能考虑这些相互作用的现有方法。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014195047053.png"
                     
                ></p>
<h4 id="FG-OrIU"><a href="#FG-OrIU" class="headerlink" title="FG-OrIU"></a>FG-OrIU</h4><p><em>FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning</em></p>
<ul>
<li>暂无公开pdf</li>
<li><a class="link"   target="_blank" rel="noopener" href="https://iccv.thecvf.com/virtual/2025/poster/2627" >poster<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><blockquote>
<p>Incremental unlearning (IU) 对于预训练模型满足顺序数据删除要求至关重要，但现有方法主要抑制参数或混淆知识，而没有在特征和梯度层面进行明确的约束，导致 <strong>superficial forgetting（表面遗忘），残留信息仍然可恢复。</strong><br>这种不完全遗忘存在安全漏洞风险，并破坏了保留平衡，尤其是在 IU 场景中。<br>我们提出了 <strong>FG-OrIU（Feature-Gradient Orthogonality for Incremental Unlearning）</strong> ，这是第一个 <strong>统一特征和梯度层面正交约束以实现深度遗忘的框架，其中遗忘效应是不可逆的。</strong><br>FG-OrIU <strong>通过奇异值分解 (SVD) 对特征空间进行分解，将遗忘特征和剩余类别特征分离到不同的子空间中。</strong><br>然后，它强制实施双重约束：对遗忘类和剩余类进行 <strong>特征正交投影</strong> ，而 <strong>梯度正交投影</strong> 则防止在更新过程中重新引入遗忘知识并干扰剩余类。<br>此外，<strong>动态子空间自适应</strong> 会合并新遗忘的子空间并收缩剩余子空间，从而确保在连续的反学习任务中，移除和保留之间保持稳定的平衡。<br>大量实验证明了我们方法的有效性。</p>
</blockquote>
</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20251014195432896.png"
                     
                ></p>

		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> Paper Collection of Safe Diffusion</li>
        <li><strong>Author:</strong> LeoJeshua</li>
        <li><strong>Created at
                :</strong> 2024-12-21 15:49:09</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2025-11-10 16:42:08
            </li>
        
        <li>
            <strong>Link:</strong> https://leojeshua.github.io/DMs/Paper-Collection-of-Safe-Diffusion/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

		</div>
		

		
		<ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
			
			<li class="tag-item mx-0.5">
				<a href="/tags/DMs/">#DMs</a>&nbsp;
			</li>
			
			<li class="tag-item mx-0.5">
				<a href="/tags/Paper-Collection/">#Paper Collection</a>&nbsp;
			</li>
			
		</ul>
		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			<div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="prev" rel="prev" href="/Course/eecs498/eecs498/">
					<span class="left arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-left"></i>
					</span>
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">Notes of &#34;EECS498/598 Deep Learning for Computer Vision (FA2019)&#34;</span>
						<span class="post-nav-item">Prev posts</span>
					</span>
				</a>
			</div>
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/CV/Patch-Level-Adversarial-Attacks-in-Computer-Vision/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">Patch-Level Adversarial Attacks on Computer Vision</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">On this page</div>
		<div class="page-title">Paper Collection of Safe Diffusion</div>
		<ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#NeurIPS2024"><span class="nav-text">NeurIPS2024</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attack"><span class="nav-text">Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Backdoors-Attack"><span class="nav-text">Backdoors Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BiBadDiff"><span class="nav-text">BiBadDiff</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-Attack"><span class="nav-text">Adversarial Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PAP"><span class="nav-text">PAP</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdvAD"><span class="nav-text">AdvAD</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MIA-Member-Inference-Attack"><span class="nav-text">MIA (Member Inference Attack)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CLiD"><span class="nav-text">CLiD</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Watermark"><span class="nav-text">Watermark</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ZoDiac"><span class="nav-text">ZoDiac</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Federated-Learning"><span class="nav-text">Federated Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DataStealing"><span class="nav-text">DataStealing</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Defence"><span class="nav-text">Defence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Anti-Adversarial-Prompt"><span class="nav-text">Anti-Adversarial Prompt</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GuardT2I"><span class="nav-text">GuardT2I</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unlearn"><span class="nav-text">Unlearn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AdvUnlearn"><span class="nav-text">AdvUnlearn</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Leveraging-Catastrophic-Forgetting"><span class="nav-text">Leveraging Catastrophic Forgetting</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Memory-Concept-Location"><span class="nav-text">Memory&#x2F;Concept Location</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Finding-NeMo"><span class="nav-text">Finding NeMo</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#P-ESD-P-AC-SGAW-Wrokshop"><span class="nav-text">P-ESD&#x2F;P-AC [SGAW Wrokshop]</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CVPR-2025"><span class="nav-text">CVPR 2025</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attack-1"><span class="nav-text">Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-Attack-1"><span class="nav-text">Adversarial Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FastProtect"><span class="nav-text">FastProtect</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I2VGuard"><span class="nav-text">I2VGuard</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Poisoning-Attack"><span class="nav-text">Data Poisoning Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Silent-Branding-Attack"><span class="nav-text">Silent Branding Attack</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MIA"><span class="nav-text">MIA</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CDI-Copyrighted-Data-Identification"><span class="nav-text">CDI (Copyrighted Data Identification)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bias"><span class="nav-text">Bias</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#IBI-Implicit-Bias-Injection-Attacks"><span class="nav-text">IBI (Implicit Bias Injection Attacks)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Watermarks"><span class="nav-text">Watermarks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Black-Box-Forgery-Attacks-on-Semantic-Watermarks"><span class="nav-text">Black-Box Forgery Attacks on Semantic Watermarks</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Defence-1"><span class="nav-text">Defence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Guidence"><span class="nav-text">Guidence</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DAG-Detect-and-Guide"><span class="nav-text">DAG (Detect-and-Guide)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Concept-Replacer"><span class="nav-text">Concept Replacer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unlern"><span class="nav-text">Unlern</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#EraseDiff"><span class="nav-text">EraseDiff</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Localized-Concept-Erasure-GLoCE"><span class="nav-text">Localized Concept Erasure (GLoCE)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ACE-Anti-editing-Concept-Erasure"><span class="nav-text">ACE (Anti-editing Concept Erasure)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#STEREO-Search-Thoroughly-Enough-Robustly-Erase-Once"><span class="nav-text">STEREO (Search Thoroughly Enough, Robustly Erase Once)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RIIDL-Responsible-Interpretable-Intermediate-Diffusion-Latents"><span class="nav-text">RIIDL (Responsible Interpretable Intermediate Diffusion Latents)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdaVD-Adaptive-Vaule-Decomposer"><span class="nav-text">AdaVD (Adaptive Vaule Decomposer)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FADE-Fine-grained-Attenuation-for-Diffusion-Erasure"><span class="nav-text">FADE (Fine-grained Attenuation for Diffusion Erasure)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TIU-The-Illusion-of-Unlearning"><span class="nav-text">TIU (The Illusion of Unlearning)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Six-CD-Benchmark"><span class="nav-text">Six-CD Benchmark</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bias-1"><span class="nav-text">Bias</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MPR-Multi-group-Proportional-Representation"><span class="nav-text">MPR (Multi-group Proportional Representation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Rethinking-Training-for-De-biasing-Text-to-Image-Generation"><span class="nav-text">Rethinking Training for De-biasing Text-to-Image Generation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Watermarks-1"><span class="nav-text">Watermarks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SleeperMark"><span class="nav-text">SleeperMark</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ICML-2025"><span class="nav-text">ICML 2025</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attack-2"><span class="nav-text">Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-Attack-2"><span class="nav-text">Adversarial Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AdvI2I"><span class="nav-text">AdvI2I</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DiffAdvMAP"><span class="nav-text">DiffAdvMAP</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Anti-Protective-Perturbation"><span class="nav-text">Anti - Protective Perturbation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CAT-Contrastive-Adversarial-Training"><span class="nav-text">CAT (Contrastive Adversarial Training)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Defence-2"><span class="nav-text">Defence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unlearn-1"><span class="nav-text">Unlearn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SAeUron"><span class="nav-text">SAeUron</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaptive-Median-Smoothing"><span class="nav-text">Adaptive Median Smoothing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Concept-Mechanism"><span class="nav-text">Concept Mechanism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Concept-Reachability"><span class="nav-text">Concept Reachability</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ConceptAttention"><span class="nav-text">ConceptAttention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mechanisms-of-Projective-Composition-of-Diffusion-Models"><span class="nav-text">Mechanisms of Projective Composition of Diffusion Models</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Can-Diffusion-Models-Learn-Hidden-Inter-Feature-Rules-Behind-Images"><span class="nav-text">Can Diffusion Models Learn Hidden Inter-Feature Rules Behind Images?</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Privacy"><span class="nav-text">Privacy</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Ambient-Diffusion"><span class="nav-text">Ambient Diffusion</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Steganography"><span class="nav-text">Steganography</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MDDM"><span class="nav-text">MDDM</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other"><span class="nav-text">Other</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-Purification"><span class="nav-text">Adversarial Purification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Diffusion-based-Adversarial-Purification-from-the-Perspective-of-the-Frequency-Domain"><span class="nav-text">Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unlearn-2"><span class="nav-text">Unlearn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Not-All-Wrong-is-Bad"><span class="nav-text">Not All Wrong is Bad</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Knowledge-Swapping-via-Learning-and-Unlearning"><span class="nav-text">Knowledge Swapping via Learning and Unlearning</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ICCV-2025"><span class="nav-text">ICCV 2025</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attack-3"><span class="nav-text">Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-Attack-3"><span class="nav-text">Adversarial Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DIA-DDIM-Inversion-Attack"><span class="nav-text">DIA (DDIM Inversion Attack)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ZIUM-Zero-shot-Intent-aware-adversarial-attack-on-Unlearned-Models"><span class="nav-text">ZIUM (Zero-shot Intent-aware adversarial attack on Unlearned Models)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Backdoor-Attack"><span class="nav-text">Backdoor Attack</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BadVideo"><span class="nav-text">BadVideo</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Defence-3"><span class="nav-text">Defence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Anomaly-Detection-for-Adversarial-and-Backdoor-Attacks"><span class="nav-text">Anomaly Detection for Adversarial and Backdoor Attacks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DADet-Diffusion-Anomaly-Detection"><span class="nav-text">DADet (Diffusion Anomaly Detection)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unlearn-3"><span class="nav-text">Unlearn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TRCE-Towards-Reliable-Malicious-Concept-Erasure"><span class="nav-text">TRCE (Towards Reliable Malicious Concept Erasure)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SuMa-Subspace-Mapping"><span class="nav-text">SuMa (Subspace Mapping)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Meta-Unlearning"><span class="nav-text">Meta-Unlearning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EraseBench"><span class="nav-text">EraseBench</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Holistic-Unlearning-Benchmark"><span class="nav-text">Holistic Unlearning Benchmark</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Watermark-1"><span class="nav-text">Watermark</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PlugMark"><span class="nav-text">PlugMark</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Copyright"><span class="nav-text">Copyright</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CopyrightShield"><span class="nav-text">CopyrightShield</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-1"><span class="nav-text">Other</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Unlearn-4"><span class="nav-text">Unlearn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MUNBa-Machine-Unlearning-via-Nash-Bargaining"><span class="nav-text">MUNBa (Machine Unlearning via Nash Bargaining)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Water4MU"><span class="nav-text">Water4MU</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LUR-Learning-to-Unlearn-while-Retaining"><span class="nav-text">LUR (Learning to Unlearn while Retaining)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#FG-OrIU"><span class="nav-text">FG-OrIU</span></a></li></ol></li></ol></li></ol></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2024</span>
              -
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-cog fa-spin" style="--fa-animation-duration:15s"></i>&nbsp;&nbsp;<a href="/">LeoJeshua</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        15 posts in total
                    </span>
                    
                        <span>
                            40.2k words in total
                        </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.5</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
            
                
        
                
        
                

                    
                        <script data-swup-reload-script>var url_1736="https://api.cdnorg.cn:666";var token_1736="8d393e3086e1d2d48267460f67b7bf8d1a09f23f231d32de4f3fb3a12f3af020";var cltj_1736=document.createElement("script");cltj_1736.src=url_1736+"/tj/tongji.js?v=2.201";var s_1736=document.getElementsByTagName("script")[0];s_1736.parentNode.insertBefore(cltj_1736,s_1736);</script>
                    
        
                

                    
                        <script data-swup-reload-script type="text/javascript">(function(){var baidu=document.createElement("script");baidu.src="//i.6v6.work/v/?uid=388547";var cnzz=document.getElementsByTagName("script")[0];cnzz.parentNode.insertBefore(baidu,cnzz)})();</script>
                    
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	

</main>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Swup.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupSlideTheme.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScriptsPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupProgressPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupScrollPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/SwupPreloadPlugin.min.js" ></script>
<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/imageViewer.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/utils.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/main.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/navbarShrink.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/scrollTopBottom.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/lightDarkSwitch.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/categoryList.js" ></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/codeBlock.js" ></script>



    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/lazyload.js" ></script>



    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/runtime.js" ></script>
    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/odometer.min.js" ></script>
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/assets/odometer-theme-minimal.css">



  <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/Typed.min.js" ></script>
  <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/typed.js" ></script>







    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/anime.min.js" ></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/tools/tocToggle.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/toc.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/plugins/tabs.js" data-swup-reload-script></script>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/libs/moment-with-locales.min.js" data-swup-reload-script></script>
<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.5/files/source/js/build/layouts/essays.js" data-swup-reload-script></script>




	
</body>

</html>