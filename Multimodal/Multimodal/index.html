<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="LeoJeshua">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
        
        
        
            <link rel="preconnect" href="https://registry.npmmirror.com" crossorigin>
        
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://leojeshua.github.io/multimodal/multimodal/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="多模态(Multimodal) 及 多模态大语言模型(MLLMs) 学习笔记">
<meta property="og:url" content="https://leojeshua.github.io/Multimodal/Multimodal/index.html">
<meta property="og:site_name" content="The Blog of LeoJeshua">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://leojeshua.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2025-05-19T03:31:00.000Z">
<meta property="article:modified_time" content="2025-07-25T15:12:41.961Z">
<meta property="article:author" content="LeoJeshua">
<meta property="article:tag" content="Multimodal">
<meta property="article:tag" content="MLLMs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://leojeshua.github.io/images/redefine-og.webp">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/avatar.png" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.png">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/avatar.png">
    <!--- Page Info-->
    
    <title>
        
            多模态(Multimodal) 及 多模态大语言模型(MLLMs) 学习笔记 | Academic Blog of LeoJeshua
        
    </title>

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/fonts/Chillax/chillax.css">

    <!--- Inject Part-->
    
        
            
    
            
    

    
<link rel="stylesheet" href="/css/style.css">


    
        <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/css/build/tailwind.css">
    

    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/fonts/GeistMono/geist-mono.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/fonts/Geist/geist.css">
    <!--- Font Part-->
    
    
    
    
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"leojeshua.github.io","root":"/","language":"en"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"2px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.5rem","h3":"1.8rem","h4":"1.3rem","h5":"1.2rem","h6":"1.1rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":4,"number":false,"expand":true,"init_open":true},"copyright":{"enable":true,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"light"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1200px","sidebar_width":"30%","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"static","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"从-1开始的科研训练之路","subtitle":{"text":["If you shed tears when you miss the sun, you also miss the stars.","Reading maketh a full man; conference a ready man; and writing an exact man. (Francis Bacon)","Who drives me forward like fate? The Myself striding on my back."],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/LeoJeshua","instagram":null,"zhihu":null,"twitter":null,"email":"jiaxu.liu.ai@gmail.com"},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.2","navbar":{"auto_hide":true,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Categories":{"path":"/categories/","icon":"fa-regular fa-folder"},"Tags":{"icon":"fa-regular fa-tags","path":"/tags/"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"cloud"},"home":{"sidebar":{"enable":true,"position":"right","first_item":"menu","announcement":"Aspiring to contribute to cutting-edge AI security research with a focus on offensive strategies to uncover and mitigate AI vulnerabilities.","show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2024/12/20 21:33:00"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/fontawesome/fontawesome.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/fontawesome/brands.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/fontawesome/solid.min.css">
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/fontawesome/regular.min.css">
    
    
    
    
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>



<body>
	<div class="progress-bar-container">
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Academic Blog of LeoJeshua
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/archives"
                                        >
                                    <i class="fa-regular fa-archive fa-fw"></i>
                                    ARCHIVES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/categories/"
                                        >
                                    <i class="fa-regular fa-folder fa-fw"></i>
                                    CATEGORIES
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/tags/"
                                        >
                                    <i class="fa-regular fa-tags fa-fw"></i>
                                    TAGS
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/archives"
                        >
                            <span>
                                ARCHIVES
                            </span>
                            
                                <i class="fa-regular fa-archive fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/categories/"
                        >
                            <span>
                                CATEGORIES
                            </span>
                            
                                <i class="fa-regular fa-folder fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/tags/"
                        >
                            <span>
                                TAGS
                            </span>
                            
                                <i class="fa-regular fa-tags fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">19</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">11</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">15</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			
			
			<img src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250519113741236.png" alt="多模态(Multimodal) 及 多模态大语言模型(MLLMs) 学习笔记" class="w-full h-60 sm:h-72 md:h-80 object-cover sm:rounded-t-large dark:brightness-75" />
			
			<div class="w-full flex items-center absolute bottom-0 justify-start">
				<h1 class="article-title-cover text-center mx-6 my-6 text-second-text-color bg-background-color-transparent px-4 py-3 text-3xl sm:text-4xl md:text-5xl font-semibold backdrop-blur-lg rounded-xl border border-border-color ">多模态(Multimodal) 及 多模态大语言模型(MLLMs) 学习笔记</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/images/avatar.png">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">LeoJeshua</span>
					
					<span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv2</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2025-05-19 11:31</span>
        <span class="mobile">2025-05-19 11:31</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2025-07-25 23:12:41</span>
            <span class="mobile">2025-07-25 23:12:41</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                
                    
                        
                        <li>
                            <a href="/categories/AI/">AI</a>&nbsp;
                        </li>
                    
                    
                
                    
                        
                            <li>></li>
                        
                        <li>
                            <a href="/categories/AI/Foundations/">Foundations</a>&nbsp;
                        </li>
                    
                    
                
                    
                        
                            <li>></li>
                        
                        <li>
                            <a href="/categories/AI/Foundations/Multimodal/">Multimodal</a>&nbsp;
                        </li>
                    
                    
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/Multimodal/">Multimodal</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/MLLMs/">MLLMs</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>2k Words</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>9 Mins</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<h1 id="多模态基础-Multimodal"><a href="#多模态基础-Multimodal" class="headerlink" title="多模态基础 | Multimodal"></a>多模态基础 | Multimodal</h1><p><strong>Keywords：</strong></p>
<ul>
<li>早期<ul>
<li>VLP(Vision-and-Language Pre-training) | “视觉-语言”预训练</li>
<li>VLMs(Vision Language Models) | 视觉语言模型</li>
</ul>
</li>
<li>当前：<ul>
<li>Multimodal Pre-training | 多模态预训练</li>
</ul>
</li>
</ul>
<p><strong>Architectures:</strong></p>
<ul>
<li>dual-encoder | 双编码器<ul>
<li>适合理解任务，侧重模态特征提取(Features Extraction)</li>
<li>eg: CLIP</li>
</ul>
</li>
<li>encoder-decoder | 编码器-解码器<ul>
<li>适合生成任务，侧重模态交互(Modality Interaction)</li>
<li>eg: SimVLM, AlBeF, BLIP</li>
</ul>
</li>
<li>fusion-encoder | 混合编码器<ul>
<li>eg: ViLT, VLMo, BEiT-3</li>
</ul>
</li>
</ul>
<p><strong>Tasks:</strong></p>
<ul>
<li>Vision</li>
<li>Language</li>
<li>Vision-Language</li>
</ul>
<h2 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials"></a>Tutorials</h2><ul>
<li><a class="link" target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16o5QzKEBw/">bilibili: DeepFinder/多模态经典论文集<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><ul>
<li>配套博客：<a class="link" target="_blank" rel="noopener" href="https://www.gnn.club/?cat=29">gnn.club: 多模态算法专栏<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
</li>
</ul>
<h2 id="Milestones"><a href="#Milestones" class="headerlink" title="Milestones"></a>Milestones</h2><h3 id="CLIP-2102"><a href="#CLIP-2102" class="headerlink" title="CLIP (2102)"></a>CLIP (2102)</h3><p><em>Learning Transferable Visual Models From Natural Language Supervision</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250519113741236.png"></p>
<h3 id="ViLT-2102"><a href="#ViLT-2102" class="headerlink" title="ViLT (2102)"></a>ViLT (2102)</h3><p><em>ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.03334">https://arxiv.org/abs/2102.03334<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="AlBeF-2107"><a href="#AlBeF-2107" class="headerlink" title="AlBeF (2107)"></a>AlBeF (2107)</h3><p><em>Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.07651">https://arxiv.org/abs/2107.07651<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="VLMo-2111"><a href="#VLMo-2111" class="headerlink" title="VLMo (2111)"></a>VLMo (2111)</h3><p><em>VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.02358">https://arxiv.org/abs/2111.02358<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="BLIP-2201"><a href="#BLIP-2201" class="headerlink" title="BLIP (2201)"></a>BLIP (2201)</h3><p><em>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.12086">https://arxiv.org/abs/2201.12086<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="CoCa-2205"><a href="#CoCa-2205" class="headerlink" title="CoCa (2205)"></a>CoCa (2205)</h3><p><em>CoCa: Contrastive Captioners are Image-Text Foundation Models</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.01917">https://arxiv.org/abs/2205.01917<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="BEiT-3-2208"><a href="#BEiT-3-2208" class="headerlink" title="BEiT-3 (2208)"></a>BEiT-3 (2208)</h3><p><em>Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.10442">https://arxiv.org/abs/2208.10442<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h3 id="Uni-perceiver-2112"><a href="#Uni-perceiver-2112" class="headerlink" title="Uni-perceiver (2112)"></a>Uni-perceiver (2112)</h3><p><em>Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks</em><br><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.01522">https://arxiv.org/abs/2112.01522<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<h1 id="多模态大语言模型-MLLMs"><a href="#多模态大语言模型-MLLMs" class="headerlink" title="多模态大语言模型 | MLLMs"></a>多模态大语言模型 | MLLMs</h1><p><strong>Keywords</strong>:</p>
<ul>
<li>LMMs(Large Multimodal Models) | 大多模态模型</li>
<li>MLLMs(Multimodal Large Language Models) | 多模态大语言模型</li>
</ul>
<h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><h3 id="MM-LLMs-2024-05"><a href="#MM-LLMs-2024-05" class="headerlink" title="MM-LLMs (2024.05)"></a>MM-LLMs (2024.05)</h3><p><em>MM-LLMs: Recent Advances in MultiModal Large Language Models</em></p>
<ul>
<li>arXiv:2401.13601v5 [cs.CL] 28 May 2024</li>
<li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2401.13601">https://arxiv.org/pdf/2401.13601<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>web: <a class="link" target="_blank" rel="noopener" href="https://mm-llms.github.io/">https://mm-llms.github.io/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> ⭐<ul>
<li>Recent Advances in MLLMs</li>
</ul>
</li>
</ul>
<p><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608235952422.png"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609000530667.png"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609165226872.png"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609170005362.png"></p>
<h3 id="A-Survey-on-MLLMs-2024-11"><a href="#A-Survey-on-MLLMs-2024-11" class="headerlink" title="A Survey on MLLMs (2024.11)"></a>A Survey on MLLMs (2024.11)</h3><p><em>A Survey on Multimodal Large Language Models</em></p>
<ul>
<li>arXiv:2306.13549v4 [cs.CV] 29 Nov 2024</li>
<li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.13549">https://arxiv.org/abs/2306.13549<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><ul>
<li>The first comprehensive survey for Multimodal Large Language Models (MLLMs). ✨</li>
</ul>
</li>
<li>code: <a class="link" target="_blank" rel="noopener" href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250519122552717.png" alt="MiG: Multimodal intelligence Group | 南京大学-多模态智能（米格）小组"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250519121758702.png" alt="A timeline of representative MLLMs"></li>
</ul>
<h3 id="MME-Survey-2024-12"><a href="#MME-Survey-2024-12" class="headerlink" title="MME-Survey (2024.12)"></a>MME-Survey (2024.12)</h3><p><em>MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs</em></p>
<ul>
<li>arXiv:2411.15296v2 [cs.CV] 8 Dec 2024</li>
<li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.15296">https://arxiv.org/abs/2411.15296<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<p><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609164810582.png"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609164641654.png"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609164700803.png"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609164428972.png"><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250609164252868.png"></p>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><h3 id="LLaVA系列"><a href="#LLaVA系列" class="headerlink" title="LLaVA系列"></a>LLaVA系列</h3><ul>
<li>blog: <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/">https://llava-vl.github.io/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>github: <a class="link" target="_blank" rel="noopener" href="https://github.com/haotian-liu/LLaVA">https://github.com/haotian-liu/LLaVA<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a><ul>
<li><a class="link" target="_blank" rel="noopener" href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md">Model Zoo<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
</li>
<li>datasets: <a class="link" target="_blank" rel="noopener" href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K">liuhaotian/LLaVA-Instruct-150K<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h4 id="LLaVA-NeurIPS’23"><a href="#LLaVA-NeurIPS’23" class="headerlink" title="LLaVA [NeurIPS’23]"></a>LLaVA [NeurIPS’23]</h4><p>paper: <em>Visual Instruction Tuning</em></p>
<ul>
<li>arXiv:2304.08485v2 [cs.CV] 11 Dec 2023</li>
<li>NeurIPS’23 Oral</li>
<li><strong>核心Idea</strong>：<ul>
<li>此前的 LLMs 通过 Instruction Tuning（指令调优）提高了zero-shot能力</li>
<li>我们提出 <code>Visual Instruction Tuning （视觉指令调优）</code>  来提高LMMs的zero-shot能力</li>
</ul>
</li>
<li>Performance：主要对标 GPT-4（闭源），超越 OpenFlamingo（开源）和 BLIP-2（开源）<ul>
<li>LLaVA+GPT-4(judge) 当时在多模态推理数据集 ScienceQA 上实现 new SOTA (92.53)</li>
</ul>
</li>
<li>模型大小：13B(default), 7B</li>
<li>训练成本：仅用单机8张A100，训练不到一天</li>
</ul>
<h5 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h5><ol>
<li><p>Multimodal Instruction-following Agents</p>
<ul>
<li>第1类：端到端模型。如 vision-language navigation task, Habitat(具身智能)；InstructPix2Pix(图像编辑)</li>
<li>第2类：多模型协作（通过LangChain）。如 Visual ChatGPT, X-GPT, MM-REACT, VisProg, ViperGPT</li>
</ul>
</li>
<li><p>Instruction Tuning</p>
<ul>
<li>LLMs：GPT-3, T5, PaLM, OPT<ul>
<li>指令微调（简单方法提高zero-shot/few-shot能力）：INstructGPT/ChatGPT, FLAN-T5, FLAN-PaLM, OPT-IML</li>
</ul>
</li>
<li>LMMs：BLIP-2, FROMAGe, KOSMOS-1, PaLM-E; Flamingo (因其强大的zero-shot迁移能力和In-Context-Learning能力，可以被视为多模态领域的GPT-3 moment)</li>
<li>OpenFlamingo, LLaMA-Adapter 让 recent “best” open-source LLM LLaMA 能接收image输入（但他们没有显式的在vision-language instruction data上微调，并且在多模态任务上通常表现下降）</li>
</ul>
</li>
</ol>
<h5 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h5><ol>
<li>【合成数据】 <strong>Multimodal language-image instruction-following data</strong><ul>
<li>一个重要挑战是缺乏vision-language instruction-folling data，我们提出第一个使用 <code>language-only ChatGPT/GPT4</code>，从 <code>image-text pairs</code> 构造训练数据 的pipeline</li>
</ul>
</li>
<li>【模型架构】 <strong>Large Multimodal Models(LMMs)</strong><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608181154968.png"><ul>
<li>pretrained <code>Visual Encoder</code> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="3.468ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1533 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(866,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(1144,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container><ul>
<li>架构：<a class="link" target="_blank" rel="noopener" href="https://huggingface.co/openai/clip-vit-large-patch14">CLIP visual encoder ViT-L/14<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>功能：提取 input image <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.837ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1253.9 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mi" transform="translate(861,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></g></svg></mjx-container> 的视觉特征得到 visual features <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.509ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1108.9 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mi" transform="translate(716,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></g></svg></mjx-container></li>
<li>视觉特征：the grid features <strong>before/after</strong> the last Transformer layer<ul>
<li>features before last layer: 更关注 localized properties，有助于模型理解specific image details ⭐（效果更好）</li>
<li>features after last layer: 更关注 global and abstract image properties</li>
</ul>
</li>
</ul>
</li>
<li><code>projection layer</code> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container><ul>
<li>架构：一个简单的线性层 ⭐（将来也可以考虑尝试gated cross-attention in Flamingo 或者 Q-former in BLIP-2）</li>
<li>功能：映射 visual features <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.509ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1108.9 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mi" transform="translate(716,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></g></svg></mjx-container> 到LLM的word embedding space中得到 visual tokens <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.844ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 1256.9 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(864,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></g></svg></mjx-container></li>
<li>备注：后续LLaVA-1.5中叫做 <code>Vision-Language Connector</code></li>
</ul>
</li>
<li>pretrained <code>Language Decoder (LLM)</code> <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="4.639ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 2050.4 1045"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D719" d="M409 688Q413 694 421 694H429H442Q448 688 448 686Q448 679 418 563Q411 535 404 504T392 458L388 442Q388 441 397 441T429 435T477 418Q521 397 550 357T579 260T548 151T471 65T374 11T279 -10H275L251 -105Q245 -128 238 -160Q230 -192 227 -198T215 -205H209Q189 -205 189 -198Q189 -193 211 -103L234 -11Q234 -10 226 -10Q221 -10 206 -8T161 6T107 36T62 89T43 171Q43 231 76 284T157 370T254 422T342 441Q347 441 348 445L378 567Q409 686 409 688ZM122 150Q122 116 134 91T167 53T203 35T237 27H244L337 404Q333 404 326 403T297 395T255 379T211 350T170 304Q152 276 137 237Q122 191 122 150ZM500 282Q500 320 484 347T444 385T405 400T381 404H378L332 217L284 29Q284 27 285 27Q293 27 317 33T357 47Q400 66 431 100T475 170T494 234T500 282Z"></path></g></g><g data-mml-node="mo" transform="translate(994.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(1383.4,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(1661.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container><ul>
<li>架构：<a class="link" target="_blank" rel="noopener" href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> (小羊驼，在语言任务上的instrction following能力是当时开源模型中最好的)</li>
<li>功能：提供 instruction following 和 reasoning 能力</li>
</ul>
</li>
</ul>
</li>
<li>【新的基准】 <strong>Multimodal instruction</strong><ul>
<li>我们提出 <code>LLaVA-Bench (COCO)</code> 和 <code>LLaVA-Bench (In-the-Wild)</code> 作为第一个量化评估LMMs的 visual instruciton following 能力 的benchmark</li>
</ul>
</li>
<li>【全开源】 <strong>Open-source</strong><ul>
<li>the generated data</li>
<li>the model checkpoints</li>
<li>a visual chat demo</li>
</ul>
</li>
</ol>
<h5 id="Model-Training-two-stage"><a href="#Model-Training-two-stage" class="headerlink" title="Model Training (two-stage)"></a>Model Training (two-stage)</h5><p>（Visual Encoder一直是冻结的）<br><strong>Stage 1: Pre-training for Feature Alignment</strong><br>冻结Visual Encoder和LLM，仅训练 <code>projection layer</code> -&gt; 使Visual Encoder兼容LLM，从而visual tokens能对齐pretrained LLM的词向量</p>
<p><strong>Stage 2: Fine-tuning End-to-End</strong><br>更新 the pre-trained weight of <code>the projection layer</code> and <code>LLM</code></p>
<h5 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h5><p><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608181750040.png"></p>
<p><strong>1.Multimodal Chatbot</strong><br>定性评估<br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608182659485.png"><br>定量评估（在LLaVA-Bench上）<br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608182833251.png"></p>
<p><strong>2.SinceQA</strong><br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608183000003.png"></p>
<h5 id="Ablation"><a href="#Ablation" class="headerlink" title="Ablation"></a>Ablation</h5><ol>
<li>视觉特征：feature before last layer（关注局部特征） &gt; feature after last year（关注全局和抽象特征）</li>
<li>思维链(Chain-of-Thought): answer-first（收敛慢，效果好） &gt; reasoning-first（收敛快，效果差）</li>
<li>预训练：有pre-training &gt; 无pre-training</li>
<li>模型大小：13B &gt; 7B</li>
</ol>
<p><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608224224913.png"></p>
<h4 id="LLaVA-1-5-CVPR’24"><a href="#LLaVA-1-5-CVPR’24" class="headerlink" title="LLaVA-1.5 [CVPR’24]"></a>LLaVA-1.5 [CVPR’24]</h4><p>paper: <em>Improved Baselines with Visual Instruction Tuning</em></p>
<ul>
<li>arXiv:2310.03744v2 [cs.CV] 15 May 2024</li>
<li>CVPR’24 (highlight)<br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250606205427068.png"></li>
<li><strong>LLaVA 对比 InstructBLIP/Qwen-VL</strong><ul>
<li>LLaVA <strong>预训练</strong> 了一个 <code>MLP cross-modal connector</code>，并 <strong>微调</strong> connector 和 LLM<ul>
<li>数据：在学术任务相关数据上，如 VQA </li>
<li>不足：short-form answers (e.g. single-word)</li>
</ul>
</li>
<li>InstructBLIP or Qwen-VL <strong>预训练</strong> 了一个 <code>visual resamplers (e.g. Qformer)</code>，并只 <strong>微调</strong> instruction-aware Qformer<ul>
<li>数据：在数亿(hundreds of millions, 129M, InstructBLIP) or 甚至数十亿(even billions, 1.4B, Qwen-VL) 的 image-text paired 数据上</li>
<li>不足：long-form conversation (overfit short-form)</li>
</ul>
</li>
</ul>
</li>
<li><strong>改进模型架构：对LLaVA的简单修改</strong><ul>
<li>Visual Encoder: <code>CLIP-ViT-L-336px</code> (the highest resolution available for CLIP)</li>
<li>Vision-Language Connector: MLP projection (从 1-layer MLP 变成 <code>2-layer MLP</code>)</li>
</ul>
</li>
<li><strong>一些开放问题：</strong><ol>
<li><strong>Scaling to high-resolution image inputs</strong> | 兼容更高分辨率<ul>
<li>采用 <em>“split-encode-merge”</em> 操作（训练得到 <code>LLaVA-1.5-HD </code> 模型）<br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608215748082.png"></li>
</ul>
</li>
<li><strong>Data efficiency</strong> | 训练数据效率<ul>
<li>通过随机下采样训练集来提高数据效率（尝试下采样率0.1-0.5），证实了和其他多模态模型一样，具有 <em>less-is-more</em> benefit<br><img lazyload="" src="/images/loading.svg" data-src="https://raw.githubusercontent.com/LeoJeshua/PicGo/main/images/20250608220018854.png"> <ul>
<li>发现下采样为原始数据的50%时，模型在full dataset上保持原始表现的98%</li>
<li>发现下采样为原始数据的30%时，模型的表现依然稳定</li>
</ul>
</li>
</ul>
</li>
<li><strong>Hallucination in LMMs</strong> | 幻觉问题<ul>
<li>model的hallucination 可能来自于 训练集的errors或hallucination </li>
<li>当input的resolution变高，幻觉减少</li>
<li>需要 【更详细的data annotation】 和 【正确处理信息的model】</li>
</ul>
</li>
<li><strong>Compositional capabilities</strong> | 组合能力(1+1&gt;2)<ul>
<li>证据：<ol>
<li>ShareGPT data 同时促进了 multimodal multilingual capability</li>
<li>academic-task-oriented datasets 同时促进了 visual groundness 能力</li>
</ol>
</li>
<li>问题：对于需要一定能力组合的某些task，仍然难以实现理想的表现<ol>
<li>例如能够正确回答VQA中某个对象的属性，不能保证在整个图像的详细说明中准确描述该对象属性</li>
<li>此外，与某些外语（例如韩语）进行对话的能力仍然落后。</li>
</ol>
</li>
</ul>
</li>
</ol>
</li>
<li>模型大小：13B(default)，7B</li>
<li>训练成本：仅用单机8张A100，~1 day</li>
</ul>
<h4 id="LLaVA-NeXT-LLaVA-1-6"><a href="#LLaVA-NeXT-LLaVA-1-6" class="headerlink" title="LLaVA-NeXT/LLaVA-1.6"></a>LLaVA-NeXT/LLaVA-1.6</h4><ul>
<li><p><a class="link" target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/model_doc/llava_next">https://huggingface.co/docs/transformers/model_doc/llava_next<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
<li><p><code>2024.01.03</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-01-30-llava-next/">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
<li><p><code>2024.04.30</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-04-30-llava-next-video/">LLaVA-NeXT: A Strong Zero-shot Video Understanding Model<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
<li><p><code>2024.05.10</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-05-10-lelava-next-stronger-llms/">LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
<li><p><code>2024.05.25</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/">LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
<li><p><code>2024.06.16</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-06-16-llava-next-interleave/">LLaVA-NeXT: Tackling Multi-image, Video, and 3D in Large Multimodal Models<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
</li>
</ul>
<h4 id="LLaVA-OneVision"><a href="#LLaVA-OneVision" class="headerlink" title="LLaVA-OneVision"></a>LLaVA-OneVision</h4><ul>
<li><code>2024.08.05</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-08-05-llava-onevision/">LLaVA-OneVision: Easy Visual Task Transfer<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h4 id="LLaVA-Video"><a href="#LLaVA-Video" class="headerlink" title="LLaVA-Video"></a>LLaVA-Video</h4><ul>
<li><code>2024.10.04</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-09-30-llava-video/">LLaVA-Video: Video Instruction Tuning with Synthetic Data<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h4 id="LLaVA-Critic"><a href="#LLaVA-Critic" class="headerlink" title="LLaVA-Critic"></a>LLaVA-Critic</h4><ul>
<li><code>2024.10.04</code> <a class="link" target="_blank" rel="noopener" href="https://llava-vl.github.io/blog/2024-10-03-llava-critic/">LLaVA-Critic: Learning to Evaluate Multimodal Models<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h3 id="Qwen-VL系列"><a href="#Qwen-VL系列" class="headerlink" title="Qwen-VL系列"></a>Qwen-VL系列</h3><ul>
<li>blog: <a class="link" target="_blank" rel="noopener" href="https://qwenlm.github.io/">https://qwenlm.github.io/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>参考资料：<ul>
<li><a class="link" target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25267823390">（姜富春）多模态技术梳理：Qwen-VL系列<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
</li>
</ul>
<h4 id="Qwen-VL"><a href="#Qwen-VL" class="headerlink" title="Qwen-VL"></a>Qwen-VL</h4><p>paper: <em>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</em></p>
<ul>
<li>arXiv:2308.12966v3 [cs.CV] 13 Oct 2023</li>
<li>blog: <a class="link" target="_blank" rel="noopener" href="https://qwenlm.github.io/zh/blog/qwen-vl/">Qwen-VL全新升级！<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>github: <a class="link" target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen-VL">https://github.com/QwenLM/Qwen-VL<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h4 id="Qwen2-VL"><a href="#Qwen2-VL" class="headerlink" title="Qwen2-VL"></a>Qwen2-VL</h4><p>paper: <em>Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution</em></p>
<ul>
<li>arXiv:2409.12191v2 [cs.CV] 3 Oct 2024</li>
<li>blog: <a class="link" target="_blank" rel="noopener" href="https://qwenlm.github.io/zh/blog/qwen2-vl/">Qwen2-VL: 更清晰地看世界<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>github: <a class="link" target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen2-VL">https://github.com/QwenLM/Qwen2-VL<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h4 id="Qwen2-5-VL"><a href="#Qwen2-5-VL" class="headerlink" title="Qwen2.5-VL"></a>Qwen2.5-VL</h4><p>report: <em>Qwen2.5-VL Technical Report</em></p>
<ul>
<li>arXiv:2502.13923v1 [cs.CV] 19 Feb 2025</li>
<li>blog: <a class="link" target="_blank" rel="noopener" href="https://qwenlm.github.io/zh/blog/qwen2.5-vl/">Qwen2.5 VL！Qwen2.5 VL！Qwen2.5 VL！<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>github: <a class="link" target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen2.5-VL">https://github.com/QwenLM/Qwen2.5-VL<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h3 id="InternVL系列"><a href="#InternVL系列" class="headerlink" title="InternVL系列"></a>InternVL系列</h3><ul>
<li>blog: <a class="link" target="_blank" rel="noopener" href="https://internvl.github.io/">https://internvl.github.io/<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li>github: <a class="link" target="_blank" rel="noopener" href="https://github.com/OpenGVLab/InternVL">https://github.com/OpenGVLab/InternVL<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h4 id="InternVL"><a href="#InternVL" class="headerlink" title="InternVL"></a>InternVL</h4><h4 id="InternVL1-5"><a href="#InternVL1-5" class="headerlink" title="InternVL1.5"></a>InternVL1.5</h4><h4 id="InternVL2-5"><a href="#InternVL2-5" class="headerlink" title="InternVL2.5"></a>InternVL2.5</h4><h4 id="InternVL2-5-MPO"><a href="#InternVL2-5-MPO" class="headerlink" title="InternVL2.5-MPO"></a>InternVL2.5-MPO</h4><h4 id="InternVL3"><a href="#InternVL3" class="headerlink" title="InternVL3"></a>InternVL3</h4><h3 id="DeepSeek-VL系列"><a href="#DeepSeek-VL系列" class="headerlink" title="DeepSeek-VL系列"></a>DeepSeek-VL系列</h3><h4 id="DeepSeek-VL"><a href="#DeepSeek-VL" class="headerlink" title="DeepSeek-VL"></a>DeepSeek-VL</h4><p><em>DeepSeek-VL: Towards Real-World Vision-Language Understanding</em></p>
<ul>
<li><a class="link" target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-VL">https://github.com/deepseek-ai/DeepSeek-VL<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.05525">https://arxiv.org/pdf/2403.05525<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h4 id="DeepSeek-VL2"><a href="#DeepSeek-VL2" class="headerlink" title="DeepSeek-VL2"></a>DeepSeek-VL2</h4><p><em>DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding</em></p>
<ul>
<li><a class="link" target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-VL2">https://github.com/deepseek-ai/DeepSeek-VL2<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2412.10302">https://arxiv.org/pdf/2412.10302<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><h2 id="Leaderboard"><a href="#Leaderboard" class="headerlink" title="Leaderboard"></a>Leaderboard</h2><h3 id="司南-OpenCompass"><a href="#司南-OpenCompass" class="headerlink" title="司南 OpenCompass"></a>司南 OpenCompass</h3><ul>
<li><a class="link" target="_blank" rel="noopener" href="https://rank.opencompass.org.cn/leaderboard-multimodal-official/?m=REALTIME">多模态模型官方自建榜单<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
<li><a class="link" target="_blank" rel="noopener" href="https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME">多模态模型公开学术榜单<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>

		</div>

		
		<div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8">
			<div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> 多模态(Multimodal) 及 多模态大语言模型(MLLMs) 学习笔记</li>
        <li><strong>Author:</strong> LeoJeshua</li>
        <li><strong>Created at
                :</strong> 2025-05-19 11:31:00</li>
        
            <li>
                <strong>Updated at
                    :</strong> 2025-07-25 23:12:41
            </li>
        
        <li>
            <strong>Link:</strong> https://leojeshua.github.io/Multimodal/Multimodal/
        </li>
        <li>
            <strong>
                License:
            </strong>
            

            
                This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a>.
            
        </li>
    </ul>
</div>

		</div>
		

		
		<ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
			
			<li class="tag-item mx-0.5">
				<a href="/tags/Multimodal/">#Multimodal</a>&nbsp;
			</li>
			
			<li class="tag-item mx-0.5">
				<a href="/tags/MLLMs/">#MLLMs</a>&nbsp;
			</li>
			
		</ul>
		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			<div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="prev" rel="prev" href="/DMs/Video-Generation/">
					<span class="left arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-left"></i>
					</span>
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item">视频生成(Video Generation) 学习笔记</span>
						<span class="post-nav-item">Prev posts</span>
					</span>
				</a>
			</div>
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/NLP/LLM/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item">LLM(Large Language Model) 学习笔记</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">
            多模态(Multimodal) 及 多模态大语言模型(MLLMs) 学习笔记
        </div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9F%BA%E7%A1%80-Multimodal"><span class="nav-text">多模态基础 | Multimodal</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tutorials"><span class="nav-text">Tutorials</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Milestones"><span class="nav-text">Milestones</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CLIP-2102"><span class="nav-text">CLIP (2102)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ViLT-2102"><span class="nav-text">ViLT (2102)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlBeF-2107"><span class="nav-text">AlBeF (2107)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VLMo-2111"><span class="nav-text">VLMo (2111)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BLIP-2201"><span class="nav-text">BLIP (2201)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CoCa-2205"><span class="nav-text">CoCa (2205)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BEiT-3-2208"><span class="nav-text">BEiT-3 (2208)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Uni-perceiver-2112"><span class="nav-text">Uni-perceiver (2112)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-MLLMs"><span class="nav-text">多模态大语言模型 | MLLMs</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Survey"><span class="nav-text">Survey</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#MM-LLMs-2024-05"><span class="nav-text">MM-LLMs (2024.05)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Survey-on-MLLMs-2024-11"><span class="nav-text">A Survey on MLLMs (2024.11)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MME-Survey-2024-12"><span class="nav-text">MME-Survey (2024.12)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Models"><span class="nav-text">Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LLaVA%E7%B3%BB%E5%88%97"><span class="nav-text">LLaVA系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaVA-NeurIPS%E2%80%9923"><span class="nav-text">LLaVA [NeurIPS’23]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaVA-1-5-CVPR%E2%80%9924"><span class="nav-text">LLaVA-1.5 [CVPR’24]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaVA-NeXT-LLaVA-1-6"><span class="nav-text">LLaVA-NeXT&#x2F;LLaVA-1.6</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaVA-OneVision"><span class="nav-text">LLaVA-OneVision</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaVA-Video"><span class="nav-text">LLaVA-Video</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LLaVA-Critic"><span class="nav-text">LLaVA-Critic</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Qwen-VL%E7%B3%BB%E5%88%97"><span class="nav-text">Qwen-VL系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Qwen-VL"><span class="nav-text">Qwen-VL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Qwen2-VL"><span class="nav-text">Qwen2-VL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Qwen2-5-VL"><span class="nav-text">Qwen2.5-VL</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#InternVL%E7%B3%BB%E5%88%97"><span class="nav-text">InternVL系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#InternVL"><span class="nav-text">InternVL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#InternVL1-5"><span class="nav-text">InternVL1.5</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#InternVL2-5"><span class="nav-text">InternVL2.5</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#InternVL2-5-MPO"><span class="nav-text">InternVL2.5-MPO</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#InternVL3"><span class="nav-text">InternVL3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DeepSeek-VL%E7%B3%BB%E5%88%97"><span class="nav-text">DeepSeek-VL系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepSeek-VL"><span class="nav-text">DeepSeek-VL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DeepSeek-VL2"><span class="nav-text">DeepSeek-VL2</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Benchmark"><span class="nav-text">Benchmark</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Leaderboard"><span class="nav-text">Leaderboard</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%B8%E5%8D%97-OpenCompass"><span class="nav-text">司南 OpenCompass</span></a></li></ol></li></ol></li></ol>

    </div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2024</span>
              -
            
            2025&nbsp;&nbsp;<i class="fa-solid fa-cog fa-spin" style="--fa-animation-duration:15s"></i>&nbsp;&nbsp;<a href="/">LeoJeshua</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        15 posts in total
                    </span>
                    
                        <span>
                            23.4k words in total
                        </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.2</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
            
                
        
                
        
                

                    
                        <script data-swup-reload-script>var url_1736="https://api.cdnorg.cn:666";var token_1736="8d393e3086e1d2d48267460f67b7bf8d1a09f23f231d32de4f3fb3a12f3af020";var cltj_1736=document.createElement("script");cltj_1736.src=url_1736+"/tj/tongji.js?v=2.201";var s_1736=document.getElementsByTagName("script")[0];s_1736.parentNode.insertBefore(cltj_1736,s_1736);</script>
                    
        
                

                    
                        <script data-swup-reload-script type="text/javascript">(function(){var baidu=document.createElement("script");baidu.src="//i.6v6.work/v/?uid=388547";var cnzz=document.getElementsByTagName("script")[0];cnzz.parentNode.insertBefore(baidu,cnzz)})();</script>
                    
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	

</main>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/Swup.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/SwupSlideTheme.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/SwupScriptsPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/SwupProgressPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/SwupScrollPlugin.min.js" ></script><script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/SwupPreloadPlugin.min.js" ></script>
<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/tools/imageViewer.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/utils.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/main.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/layouts/navbarShrink.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/tools/scrollTopBottom.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/tools/lightDarkSwitch.js" ></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/layouts/categoryList.js" ></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/tools/codeBlock.js" ></script>



    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/layouts/lazyload.js" ></script>



    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/tools/runtime.js" ></script>
    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/odometer.min.js" ></script>
    <link rel="stylesheet" href="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/assets/odometer-theme-minimal.css">



  <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/Typed.min.js" ></script>
  <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/plugins/typed.js" ></script>







    <script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/anime.min.js" ></script>




    <script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/tools/tocToggle.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/layouts/toc.js" data-swup-reload-script></script><script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/plugins/tabs.js" data-swup-reload-script></script>


<script  src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/libs/moment-with-locales.min.js" data-swup-reload-script></script>
<script type="module" src="https://registry.npmmirror.com/hexo-theme-redefine/2.8.2/files/source/js/build/layouts/essays.js" data-swup-reload-script></script>




	
</body>

</html>